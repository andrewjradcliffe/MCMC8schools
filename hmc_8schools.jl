#
# Date created: 2022-02-08
# Author: aradclif
#
#
############################################################################################
# # Centered parameterization:
# # yâ±¼ ~ N(Î±â±¼, Ïƒâ±¼Â²)
# # Î±â±¼ ~ N(Î¼, Ï„Â²)
# # Î¼ ~ U(-âˆ, âˆ)
# # Ï„ ~ U(0, âˆ)
# # Ï„ transformed to logÏ„ to place on unconstrained space

# function gradlogÏ€(Î±, Î¼, logÏ„, y, ÏƒÂ²)
#     Ï„Â² = abs2(exp(logÏ„))
#     # âˆ‡l = @. - (Î± - y) / ÏƒÂ² - (Î± - Î¼) / Ï„Â²
#     âˆ‡l = .- (Î± .- y) ./ ÏƒÂ² .- (Î± .- Î¼) ./ Ï„Â²
#     dÏ€dÎ¼ = 0.
#     for j âˆˆ axes(Î±, 1)
#         dÏ€dÎ¼ -= (Î¼ - Î±[j]) / Ï„Â² # -
#     end
#     dÏ€dlogÏ„ = - length(Î±) + 1.
#     for j âˆˆ axes(Î±, 1)
#         dÏ€dlogÏ„ += abs2(Î±[j] - Î¼) / Ï„Â²
#     end
#     [âˆ‡l; dÏ€dÎ¼; dÏ€dlogÏ„]
# end
# gradlogÏ€(q, y, ÏƒÂ²) = gradlogÏ€(q[1:8], q[9], q[10], y, ÏƒÂ²)

# function logÏ€q(Î±, Î¼, logÏ„, y, ÏƒÂ²)
#     Ï„Â² = abs2(exp(logÏ„))
#     # s = 0
#     # s += logÏ„
#     s = logÏ„
#     for j âˆˆ axes(Î±, 1)
#         s += -log(âˆš(2Ï€ * ÏƒÂ²[j])) - 0.5abs2(y[j] - Î±[j]) / ÏƒÂ²[j] - 0.5abs2(Î±[j] - Î¼) / Ï„Â²
#         # s += - 0.5log(2Ï€ * ÏƒÂ²[j]) - 0.5abs2(y[j] - Î±[j]) / ÏƒÂ²[j] - 0.5abs2(Î±[j] - Î¼) / Ï„Â²
#     end
#     s -= length(Î±) * log(âˆš(2Ï€ * Ï„Â²))
#     # s -= length(Î±) * 0.5log(2Ï€ * Ï„Â²)
#     s
#     # # unnormalized density
#     # Ï„Â² = abs2(exp(logÏ„))
#     # s = logÏ„
#     # for j âˆˆ axes(Î±, 1)
#     #     s += -0.5abs2(y[j] - Î±[j]) / ÏƒÂ²[j] - 0.5abs2(Î±[j] - Î¼) / Ï„Â²
#     # end
#     # s += -0.5log(2Ï€ * Ï„Â²)
#     # s
# end
# logÏ€q(q, y, ÏƒÂ²) = logÏ€q(q[1:8], q[9], q[10], y, ÏƒÂ²)

# function inversediagdet(Mâ»Â¹::Matrix{T}) where {T<:Real}
#     s = one(T)
#     for j âˆˆ axes(Mâ»Â¹, 1)
#         s *= one(T) / Mâ»Â¹[j, j]
#     end
#     s
# end

# function weightedss(p, Mâ»Â¹::Matrix{T}) where {T<:Real}
#     s = zero(T)
#     for j âˆˆ axes(Mâ»Â¹, 1)
#         s += Mâ»Â¹[j, j] * abs2(p[j])
#         # or:
#         # pâ±¼ = p[j]
#         # s += pâ±¼ * pâ±¼ * Mâ»Â¹[j, j]
#     end
#     s
# end

# function logÏ€p(p, Mâ»Â¹)
#     # s = length(p) * log(2Ï€)
#     # s += log(inversediagdet(Mâ»Â¹))
#     # s += weightedss(p, Mâ»Â¹)
#     # s *= -0.5
#     # s
#     # # Or, simpler:
#     -0.5 * (length(p) * log(2Ï€) + log(inversediagdet(Mâ»Â¹)) + weightedss(p, Mâ»Â¹))
#     # # unnormalized density
#     # -0.5weightedss(p, Mâ»Â¹) -0.5log(inversediagdet(Mâ»Â¹))
#     # -0.5weightedss(p, Mâ»Â¹)
# end

# function symplecticintegrate(qâ‚€, pâ‚€, Mâ»Â¹, Ïµ, L, y, ÏƒÂ²)
#     # # Option 1
#     # qâ€², pâ€² = qâ‚€, pâ‚€
#     # pâ€² = pâ€² + 0.5Ïµ * gradlogÏ€(qâ‚€, y, ÏƒÂ²) # -
#     # for l = 2:L
#     #     qâ€² = qâ€² + Ïµ * Mâ»Â¹ * pâ€²
#     #     pâ€² = pâ€² + Ïµ * gradlogÏ€(qâ€², y, ÏƒÂ²) # -
#     # end
#     # qâ€² = qâ€² + Ïµ * Mâ»Â¹ * pâ€²
#     # pâ€² = pâ€² + 0.5Ïµ * gradlogÏ€(qâ€², y, ÏƒÂ²) # -
#     # # Option 2
#     pâ€² = pâ‚€ + 0.5Ïµ * gradlogÏ€(qâ‚€, y, ÏƒÂ²) # -
#     qâ€² = qâ‚€ + Ïµ * Mâ»Â¹ * pâ€²
#     for l = 2:L
#         pâ€² = pâ€² + Ïµ * gradlogÏ€(qâ€², y, ÏƒÂ²) # -
#         qâ€² = qâ€² + Ïµ * Mâ»Â¹ * pâ€²
#     end
#     pâ€² = pâ€² + 0.5Ïµ * gradlogÏ€(qâ€², y, ÏƒÂ²) # -
#     qâ€², -pâ€²
# end

# function logratio(qâ€², pâ€², qâ‚€, pâ‚€, Mâ»Â¹, y, ÏƒÂ²)
#     logÏ€q(qâ€², y, ÏƒÂ²) + logÏ€p(pâ€², Mâ»Â¹) - logÏ€q(qâ‚€, y, ÏƒÂ²) - logÏ€p(pâ‚€, Mâ»Â¹)
# end

# function metropolis(qâ€², pâ€², qâ‚€, pâ‚€, Mâ»Â¹, y, ÏƒÂ²)
#     r = exp(logratio(qâ€², pâ€², qâ‚€, pâ‚€, Mâ»Â¹, y, ÏƒÂ²))
#     rand() < r ? (qâ€², pâ€²) : (qâ‚€, pâ‚€)
# end

# # Using the mass matrix directly
# function momentumrand_M(M)
#     p = randn(size(M, 1))
#     @inbounds for j âˆˆ axes(M, 1)
#         p[j] *= âˆš(M[j, j])
#     end
#     p
# end

# # Using the lower triangular of the cholesky, AAáµ€ = M
# function momentumrand_A(A)
#     p = randn(size(A, 1))
#     @inbounds for j âˆˆ axes(A, 1)
#         p[j] *= A[j, j]
#     end
#     p
# end

# # Using the inverse mass matrix directly
# function momentumrand_Mâ»Â¹(Mâ»Â¹)
#     p = randn(size(Mâ»Â¹, 1))
#     @inbounds for j âˆˆ axes(Mâ»Â¹, 1)
#         p[j] *= inv(âˆš(Mâ»Â¹[j, j]))
#     end
#     p
# end

# function hmc(Ïµ, L, M, Mâ»Â¹, Î±â‚€, Î¼â‚€, logÏ„â‚€, y, ÏƒÂ²)
#     qâ‚€ = [Î±â‚€; Î¼â‚€; logÏ„â‚€]
#     pâ‚€ = momentumrand_M(M)
#     qâ€², pâ€² = symplecticintegrate(qâ‚€, pâ‚€, Mâ»Â¹, Ïµ, L, y, ÏƒÂ²)
#     metropolis(qâ€², pâ€², qâ‚€, pâ‚€, Mâ»Â¹, y, ÏƒÂ²)
# end

# function hmc(N, Ïµ, L, M, Mâ»Â¹, Î±â‚€, Î¼â‚€, logÏ„â‚€, y, ÏƒÂ²)
#     qâ‚€ = [Î±â‚€; Î¼â‚€; logÏ„â‚€]
#     # Î¸ = Vector{Vector{Float64}}()
#     Î¸ = Matrix{Float64}(undef, 10, N)
#     n, t = 0, 0
#     while n < N
#         pâ‚€ = momentumrand_M(M)
#         qâ€², pâ€² = symplecticintegrate(qâ‚€, pâ‚€, Mâ»Â¹, Ïµ, L, y, ÏƒÂ²)
#         r = exp(logÏ€q(qâ€², y, ÏƒÂ²) + logÏ€p(pâ€², Mâ»Â¹) - logÏ€q(qâ‚€, y, ÏƒÂ²) - logÏ€p(pâ‚€, Mâ»Â¹))
#         if rand() < r
#             # push!(Î¸, qâ€²)
#             qâ‚€ = qâ€²
#             n += 1
#             t += 1
#             Î¸[:, n] = qâ€²
#         else
#             t += 1
#         end
#     end
#     Î¸, n, t
# end

# function hmc(N, Ïµ, L, M, Mâ»Â¹, y, ÏƒÂ²)
#     J = length(y)
#     qâ‚€, t = initial(Ïµ, L, M, Mâ»Â¹, y, ÏƒÂ²)
#     Î±â‚€, Î¼â‚€, logÏ„â‚€ = qâ‚€[1:J], qâ‚€[J + 1], qâ‚€[J + 2]
#     hmc(N, Ïµ, L, M, Mâ»Â¹, Î±â‚€, Î¼â‚€, logÏ„â‚€, y, ÏƒÂ²)
# end

# function initial(Ïµ, L, M, Mâ»Â¹, y, ÏƒÂ²)
#     J = length(y)
#     ğ“ˆ = sqrt.(diag(Mâ»Â¹))
#     t = 0
#     while true
#         Î±â‚€ = randn(J) .* ğ“ˆ[1:J]
#         Î¼â‚€ = randn() * ğ“ˆ[J + 1]
#         logÏ„â‚€ = randn() * ğ“ˆ[J + 2]
#         qâ‚€ = [Î±â‚€; Î¼â‚€; logÏ„â‚€]
#         qâ€², pâ€² = hmc(Ïµ, L, M, Mâ»Â¹, Î±â‚€, Î¼â‚€, logÏ„â‚€, y, ÏƒÂ²)
#         t += 1
#         qâ€² != qâ‚€ && break
#     end
#     return qâ€², t
# end

# ################ Data
# y = Float64[28, 8, -3, 7, -1, 1, 18, 12];
# Ïƒ = Float64[15, 10, 16, 11, 9, 11, 10, 18];
# ÏƒÂ² = abs2.(Ïƒ);
# ################ Putting it all together
# # Initialize
# J = length(y)
# # # Stan's initialization -- far too restrictive for this model
# # Î±â‚€ = rand(J) .* 4 .- 2
# # Î¼â‚€ = rand() * 4 - 2
# # logÏ„â‚€ = log(rand() * 15)
# # # Default initialization
# scale = 15;
# # Î±â‚€ = randn(J) .* scale;
# # Î¼â‚€ = randn() * scale;
# # logÏ„â‚€ = randn();
# # Specify Ïµ, L, N
# Ïµ = 0.1
# L = 10
# N = 1000
# #
# # M = diagm(scale*(ones(10))); M[10, 10] = 1;
# # Mâ»Â¹ = diagm(1 ./ diag(M));
# # Approximate M: inverse of covariance matrix of posterior
# M = diagm(fill(1 / scale^2, 10)); M[10, 10] = 1;
# Mâ»Â¹ = diagm(1 ./ diag(M));

# # Initialization tests -- guarantee a decent starting point
# qâ‚€, t = initial(Ïµ, L, M, Mâ»Â¹, y, ÏƒÂ²)
# Î±â‚€, Î¼â‚€, logÏ„â‚€ = qâ‚€[1:J], qâ‚€[J + 1], qâ‚€[J + 2]
# ####

# # Single step, fixed initialization
# @timev q, p = hmc(Ïµ, L, M, Mâ»Â¹, Î±â‚€, Î¼â‚€, logÏ„â‚€, y, ÏƒÂ²)
# # Fixed initialization
# @timev Î¸, n, t = hmc(N, Ïµ, L, M, Mâ»Â¹, Î±â‚€, Î¼â‚€, logÏ„â‚€, y, ÏƒÂ²);

# # Auto-initialization
# @timev Î¸, n, t = hmc(N, Ïµ, L, M, Mâ»Â¹, y, ÏƒÂ²);
# acprob = n / t
# mean(Î¸, dims=2)
# mean(Î¸[:, (N >> 1):N], dims=2)

# # Multiple chains
# @timev chains = [hmc(N, Ïµ, L, M, Mâ»Â¹, y, ÏƒÂ²) for _ = 1:4];
# acprob = getindex.(chains, 2) ./ getindex.(chains, 3)
# Î¸ = cat(first.(chains)..., dims=3);
# mean(Î¸, dims=(2,3))

# # Straightforward tests
# gradlogÏ€(Î±â‚€, Î¼â‚€, logÏ„â‚€, y, ÏƒÂ²)
# @benchmark momentumrand_M(M)
# A, Aáµ€ = cholesky(M)
# momentumrand_Mâ»Â¹(Mâ»Â¹)
# @benchmark momentumrand_A(A)
# A * randn(10)
# inversediagdet(Mâ»Â¹)
# @benchmark weightedss(p, Mâ»Â¹)
# @benchmark weightedss2(p, Mâ»Â¹)

# # Trajectory test
# qâ‚€, pâ‚€ = [Î±â‚€; Î¼â‚€; logÏ„â‚€], momentumrand_M(M);
# qâ€², pâ€² = symplecticintegrate(qâ‚€, pâ‚€, Mâ»Â¹, Ïµ, L, y, ÏƒÂ²)
# logÏ€q(qâ€², y, ÏƒÂ²)
# logÏ€p(pâ€², Mâ»Â¹)
# logratio(qâ€², pâ€², qâ‚€, pâ‚€, Mâ»Â¹, y, ÏƒÂ²)


# # normpdf(x) = inv(âˆš(2Ï€)) * exp(-0.5 * x * x)
# # lognormpdf(x) = -0.5log(2Ï€) - 0.5 * x * x
# # dlognormpdf(x) = -x

# ############################################################################################
# #### 2022-02-14: non-centered parameterization
# # yâ±¼ ~ N(Î¼ + Ï„ * Î·â±¼, Ïƒâ±¼Â²)
# # Î·â±¼ ~ N(0, 1)
# # Î¼ ~ U(-âˆ, âˆ)
# # Ï„ ~ U(0, âˆ)
# # Ï„ transformed to logÏ„ to place on unconstrained space

# # Clearly not the most efficient approach -- the proper way is to compute a vector, the
# # elements of which are (y[j] - Î¼ - Ï„ * Î·[j]). Then, compute dÏ€dÎ¼ and dÏ€dlogÏ„, then
# # mutate in place to complete the computation of âˆ‡l.
# function gradlogÏ€_nc(Î·, Î¼, logÏ„, y, ÏƒÂ²)
#     Ï„ = exp(logÏ„)
#     âˆ‡l = Vector{Float64}(undef, length(Î·))
#     for j âˆˆ eachindex(Î·, y, ÏƒÂ²)
#         âˆ‡l[j] = (y[j] - Î¼ - Ï„ * Î·[j]) * Ï„ / ÏƒÂ²[j] - Î·[j]
#     end
#     dÏ€dÎ¼ = 0.0
#     for j âˆˆ eachindex(Î·, y, ÏƒÂ²)
#         dÏ€dÎ¼ += (y[j] - Î¼ - Ï„ * Î·[j]) / ÏƒÂ²[j]
#     end
#     dÏ€dlogÏ„ = 1.0
#     for j âˆˆ eachindex(Î·, y, ÏƒÂ²)
#         dÏ€dlogÏ„ += Ï„ * (y[j] - Î¼ - Ï„ * Î·[j]) * Î·[j] / ÏƒÂ²[j]
#     end
#     [âˆ‡l; dÏ€dÎ¼; dÏ€dlogÏ„]
# end
# gradlogÏ€_nc(q, y, ÏƒÂ²) = gradlogÏ€_nc(q[1:8], q[9], q[10], y, ÏƒÂ²)

# function logÏ€q_nc(Î·, Î¼, logÏ„, y, ÏƒÂ²)
#     Ï„ = exp(logÏ„)
#     s = logÏ„
#     for j âˆˆ eachindex(Î·, y, ÏƒÂ²)
#         s += -0.5(log(2Ï€ * ÏƒÂ²[j]) + (1.0 / ÏƒÂ²[j]) * abs2(y[j] - Î¼ - Ï„ * Î·[j]) + log(2Ï€) + abs2(Î·[j]))
#     end
#     s
# end
# logÏ€q_nc(q, y, ÏƒÂ²) = logÏ€q_nc(q[1:8], q[9], q[10], y, ÏƒÂ²)

# function symplecticintegrate(f_gradlogÏ€, qâ‚€, pâ‚€, Mâ»Â¹, Ïµ, L, y, ÏƒÂ²)
#     pâ€² = pâ‚€ + 0.5Ïµ * f_gradlogÏ€(qâ‚€, y, ÏƒÂ²)
#     qâ€² = qâ‚€ + Ïµ * Mâ»Â¹ * pâ€²
#     for l = 2:L
#         pâ€² = pâ€² + Ïµ * f_gradlogÏ€(qâ€², y, ÏƒÂ²)
#         qâ€² = qâ€² + Ïµ * Mâ»Â¹ * pâ€²
#     end
#     pâ€² = pâ€² + 0.5Ïµ * f_gradlogÏ€(qâ€², y, ÏƒÂ²)
#     qâ€², -pâ€²
# end

# function targetrand(Mâ»Â¹::Matrix{T}) where {T<:Real}
#     K = size(Mâ»Â¹, 1)
#     qâ‚€ = randn(K)
#     for k âˆˆ eachindex(qâ‚€)
#         qâ‚€[k] *= âˆš(Mâ»Â¹[k, k])
#     end
#     qâ‚€
# end

# function hmc_nc_transition(f_gradlogÏ€, f_logÏ€q, Ïµ, L, M, Mâ»Â¹, qâ‚€, y, ÏƒÂ²)
#     pâ‚€ = momentumrand_M(M)
#     qâ€², pâ€² = symplecticintegrate(f_gradlogÏ€, qâ‚€, pâ‚€, Mâ»Â¹, Ïµ, L, y, ÏƒÂ²)
#     r = exp(f_logÏ€q(qâ€², y, ÏƒÂ²) + logÏ€p(pâ€², Mâ»Â¹) - f_logÏ€q(qâ‚€, y, ÏƒÂ²) - logÏ€p(pâ‚€, Mâ»Â¹))
#     u = rand()
#     u < r ? (qâ€², pâ€²) : (qâ‚€, pâ‚€)
# end
# function hmc_nc_transition(f_gradlogÏ€, f_logÏ€q, Ïµ, L, M, Mâ»Â¹, y, ÏƒÂ²)
#     qâ‚€ = targetrand(Mâ»Â¹)
#     hmc_nc_transition(f_gradlogÏ€, f_logÏ€q, Ïµ, L, M, Mâ»Â¹, qâ‚€, y, ÏƒÂ²)
# end

# function hmc_nc(f_gradlogÏ€, f_logÏ€q, N, Ïµ, L, M, Mâ»Â¹, qâ‚€, y, ÏƒÂ²)
#     Î¸ = Matrix{Float64}(undef, length(qâ‚€), N)
#     n, t = 0, 0
#     while n < N
#         pâ‚€ = momentumrand_M(M)
#         qâ€², pâ€² = symplecticintegrate(f_gradlogÏ€, qâ‚€, pâ‚€, Mâ»Â¹, Ïµ, L, y, ÏƒÂ²)
#         r = exp(f_logÏ€q(qâ€², y, ÏƒÂ²) + logÏ€p(pâ€², Mâ»Â¹) - f_logÏ€q(qâ‚€, y, ÏƒÂ²) - logÏ€p(pâ‚€, Mâ»Â¹))
#         if rand() < r
#             qâ‚€ = qâ€²
#             n += 1
#             t += 1
#             Î¸[:, n] = qâ€²
#         else
#             t += 1
#         end
#     end
#     Î¸, n, t
# end

# function hmc_nc(f_gradlogÏ€, f_logÏ€q, N, Ïµ, L, M, Mâ»Â¹, y, ÏƒÂ²)
#     qâ‚€ = targetrand(Mâ»Â¹)
#     hmc_nc(f_gradlogÏ€, f_logÏ€q, N, Ïµ, L, M, Mâ»Â¹, qâ‚€, y, ÏƒÂ²)
# end

# # #### Unlike the centered parameterization, the non-centered parameterization
# # will not get stuck at low values of Ï„, hence, no need to guarantee a "decent" starting point.
# # function initial_nc(f_gradlogÏ€, f_logÏ€q, Ïµ, L, M, Mâ»Â¹, y, ÏƒÂ²)
# #     K = size(Mâ»Â¹, 1)
# #     ğ“ˆ = sqrt.(diag(Mâ»Â¹))
# #     t = 0
# #     while true
# #         qâ‚€ = randn(K) .* ğ“ˆ
# #         qâ€², pâ€² = hmc_nc_1iter(f_gradlogÏ€, f_logÏ€q, Ïµ, L, M, Mâ»Â¹, qâ‚€, y, ÏƒÂ²)
# #         t += 1
# #         qâ€² != qâ‚€ && return qâ€², t
# #     end
# #     return qâ€², t
# # end
# # # Initialization tests -- guarantee a decent starting point
# # qâ‚€, t = initial_nc(gradlogÏ€_nc, logÏ€q_nc, Ïµ, L, M, Mâ»Â¹, y, ÏƒÂ²)

# # Data
# y = Float64[28, 8, -3, 7, -1, 1, 18, 12];
# Ïƒ = Float64[15, 10, 16, 11, 9, 11, 10, 18];
# ÏƒÂ² = abs2.(Ïƒ);

# # Integration time and stepsize. Interestingly, after re-parameterization,
# # Ïµ = 0.2, L = 5 gives a reasonable acceptance rate. Ïµ = 0.1, L = 10 results in acceptance
# # rate of â‰¥ 0.99, which indicates the integration could take larger steps.
# Ïµ = 0.2
# L = 5
# N = 1000

# # Euclidean-Gaussian metric
# scale = 15;
# Mâ»Â¹ = diagm(ones(10)); Mâ»Â¹[9, 9] = scale^2;
# M = diagm(1 ./ diag(Mâ»Â¹));

# # Single step, random initialization
# qâ‚€, pâ‚€ = hmc_nc_transition(gradlogÏ€_nc, logÏ€q_nc, Ïµ, L, M, Mâ»Â¹, y, ÏƒÂ²)
# # Single step, fixed initialization
# @timev q, p = hmc_nc_transition(gradlogÏ€_nc, logÏ€q_nc, Ïµ, L, M, Mâ»Â¹, qâ‚€, y, ÏƒÂ²)
# # Fixed initialization
# @timev Î¸, n, t = hmc_nc(gradlogÏ€_nc, logÏ€q_nc, N, Ïµ, L, M, Mâ»Â¹, qâ‚€, y, ÏƒÂ²);

# # Auto-initialization
# @timev Î¸, n, t = hmc_nc(gradlogÏ€_nc, logÏ€q_nc, N, Ïµ, L, M, Mâ»Â¹, y, ÏƒÂ²);
# acprob = n / t
# mean(Î¸, dims=2)

# Ï‘ = originalparam(Î¸);
# mean(Ï‘, dims=2)

# # Multiple chains
# @timev chains = [hmc_nc(gradlogÏ€_nc, logÏ€q_nc, N, Ïµ, L, M, Mâ»Â¹, y, ÏƒÂ²) for _ = 1:4];
# acprob = getindex.(chains, 2) ./ getindex.(chains, 3)
# Î¸ = cat(first.(chains)..., dims=3);
# mean(Î¸, dims=(2,3))

# Ï‘ = originalparam(Î¸);
# mean(Ï‘, dims=(2, 3))
# # Convergence check
# neff(Ï‘)
# Rhat(Ï‘)

# # Trajectory tests
# pâ‚€ = momentumrand_M(M)
# q, p = symplecticintegrate(gradlogÏ€_nc, qâ‚€, pâ‚€, Mâ»Â¹, Ïµ, L, y, ÏƒÂ²)
# #

# # # Experiment with NamedTuple -- no change in allocations or speed
# # ğ’¹ = (; :y => y, :ÏƒÂ² => ÏƒÂ²)
# # logÏ€q_nc(q, ğ’¹::NamedTuple) = logÏ€q_nc(q[1:8], q[9], q[10], ğ’¹.y, ğ’¹.ÏƒÂ²)
# # @timev logÏ€q_nc(qâ‚€, y, ÏƒÂ²)
# # @timev logÏ€q_nc(qâ‚€, ğ’¹)

# function originalparam(Î¸::Vector{T}) where {T<:Real}
#     K = length(Î¸)
#     Ï„ = exp(Î¸[K])
#     Î¼ = Î¸[K - 1]
#     Ï‘ = Vector{promote_type(T, Float64)}(undef, K)
#     for k = 1:(K - 2)
#         Ï‘[k] = Î¼ + Ï„ * Î¸[k]
#     end
#     Ï‘[K - 1] = Î¼
#     Ï‘[K] = Ï„
#     Ï‘
# end

# function originalparam(Î¸::Matrix{T}) where {T<:Real}
#     K, S = size(Î¸)
#     Ï‘ = Matrix{promote_type(T, Float64)}(undef, K, S)
#     for s âˆˆ axes(Î¸, 2)
#         Ï„ = exp(Î¸[K, s])
#         Î¼ = Î¸[K - 1, s]
#         for k = 1:(K - 2)
#             Ï‘[k, s] = Î¼ + Ï„ * Î¸[k, s]
#         end
#         Ï‘[K - 1, s] = Î¼
#         Ï‘[K, s] = Ï„
#     end
#     Ï‘
# end
# originalparam(Î¸::Array{T, 3}) where {T<:Real} = mapslices(originalparam, Î¸, dims=(1, 2))


# ################################################################
# # Functions to support formal version, other than those provided above
# function symplecticintegrate(f_gradlogÏ€, qâ‚€, pâ‚€, Mâ»Â¹, Ïµ, L)
#     pâ€² = pâ‚€ + 0.5Ïµ * f_gradlogÏ€(qâ‚€)
#     qâ€² = qâ‚€ + Ïµ * Mâ»Â¹ * pâ€²
#     for l = 2:L
#         pâ€² = pâ€² + Ïµ * f_gradlogÏ€(qâ€²)
#         qâ€² = qâ€² + Ïµ * Mâ»Â¹ * pâ€²
#     end
#     pâ€² = pâ€² + 0.5Ïµ * f_gradlogÏ€(qâ€²)
#     qâ€², -pâ€²
# end

# ################ Thinking in terms of the Hamiltonian
# H(f_logÏ€q, q, p, Mâ»Â¹) = -f_logÏ€q(q) - logÏ€p(p, Mâ»Â¹)
# E_pq(f_logÏ€q, q, E) = E + f_logÏ€q(q)
# logÏ€p(f_logÏ€q, q, E) = E_pq(f_logÏ€q, q, E)

# function transition(f_logÏ€q, f_gradlogÏ€, qâ‚€, pâ‚€, M, Mâ»Â¹, Ïµ, L)
#     qâ€², pâ€² = symplecticintegrate(f_gradlogÏ€, qâ‚€, pâ‚€, Mâ»Â¹, Ïµ, L)
#     r = exp(-H(f_logÏ€q, qâ€², pâ€², Mâ»Â¹) + H(f_logÏ€q, qâ‚€, pâ‚€, Mâ»Â¹))
#     u = rand()
#     u < r ? (qâ€², pâ€²) : (qâ‚€, pâ‚€)
# end
# transition(f_logÏ€q, f_gradlogÏ€, qâ‚€, M, Mâ»Â¹, Ïµ, L) =
#     transition(f_logÏ€q, f_gradlogÏ€, qâ‚€, momentumrand_M(M), M, Mâ»Â¹, Ïµ, L)
# transition(f_logÏ€q, f_gradlogÏ€, M, Mâ»Â¹, Ïµ, L) =
#     transition(f_logÏ€q, f_gradlogÏ€, targetrand(Mâ»Â¹), momentumrand_M(M), M, Mâ»Â¹, Ïµ, L)

# function hmc(f_logÏ€q, f_gradlogÏ€, qâ‚€, M, Mâ»Â¹, Ïµ, L, N)
#     Î¸ = Matrix{Float64}(undef, size(M, 1), N)
#     E = Vector{Float64}(undef, N)
#     n, t = 0, 0
#     while n < N
#         pâ‚€ = momentumrand_M(M)
#         qâ€², pâ€² = symplecticintegrate(f_gradlogÏ€, qâ‚€, pâ‚€, Mâ»Â¹, Ïµ, L)
#         Eâ‚€ = H(f_logÏ€q, qâ‚€, pâ‚€, Mâ»Â¹)
#         Eâ€² = H(f_logÏ€q, qâ€², pâ€², Mâ»Â¹)
#         r = exp(-Eâ€² + Eâ‚€)
#         if rand() < r
#             n += 1
#             Î¸[:, n] = qâ€²
#             qâ‚€ = qâ€²
#             E[n] = Eâ€²
#         end
#         t += 1
#     end
#     Î¸, E, n, t
# end
# hmc(f_logÏ€q, f_gradlogÏ€, M, Mâ»Â¹, Ïµ, L, N) =
#     hmc(f_logÏ€q, f_gradlogÏ€, targetrand(Mâ»Â¹), M, Mâ»Â¹, Ïµ, L, N)

# function ebfmi(E::Vector{T}) where {T<:Real}
#     N = length(E)
#     s = zero(T)
#     # As written, this has _clear_ potential for numerical stability issues
#     for n = 2:N
#         s += abs2(E[n] - E[n - 1])
#     end
#     s / (N * var(E, corrected=false))
# end

# function microcanonicalenergy(f_logÏ€q, q, E)
#     Eq = -f_logÏ€q(q)
#     Epq = E - Epq
#     Epq, Eq
# end

# function microcanonicalenergy(f_logÏ€q, Q::Matrix{T}, E::Vector{T}) where {T<:Real}
#     N = length(E)
#     Eq = Vector{Float64}(undef, N)
#     Epq = Vector{Float64}(undef, N)
#     for (n, q) âˆˆ enumerate(eachcol(Q))
#         Eq[n] = -f_logÏ€q(q)
#         Epq[n] = E[n] - Eq[n]
#     end
#     Epq, Eq
# end

# # Guaranteed initialization for models in which small Ï„ causes sticking
# # (i.e. for centered parameterizations)
# function initialize(f_logÏ€q, f_gradlogÏ€, M, Mâ»Â¹, Ïµ, L)
#     t = 0
#     while true
#         qâ‚€, pâ‚€ = targetrand(Mâ»Â¹), momentumrand_M(M)
#         qâ€², pâ€² = transition(f_logÏ€q, f_gradlogÏ€, qâ‚€, pâ‚€, M, Mâ»Â¹, Ïµ, L)
#         t += 1
#         qâ€² != qâ‚€ && return qâ€², pâ€², t
#     end
#     return qâ€², pâ€², t
# end

# # Data
# y = Float64[28, 8, -3, 7, -1, 1, 18, 12];
# Ïƒ = Float64[15, 10, 16, 11, 9, 11, 10, 18];
# ÏƒÂ² = abs2.(Ïƒ);

# # Integration time and stepsize.
# Ïµ = 0.2
# L = 5
# N = 1000

# # Euclidean-Gaussian metric
# scale = 15;
# Mâ»Â¹ = diagm(ones(10)); Mâ»Â¹[9, 9] = scale^2;
# M = diagm(1 ./ diag(Mâ»Â¹));

# f_logÏ€ = let y = y, ÏƒÂ² = ÏƒÂ²
#     q -> logÏ€q_nc(q, y, ÏƒÂ²)
# end;
# f_gradlogÏ€ = let y = y, ÏƒÂ² = ÏƒÂ²
#     q -> gradlogÏ€_nc(q, y, ÏƒÂ²)
# end;

# @benchmark transition(f_logÏ€, f_gradlogÏ€, M, Mâ»Â¹, Ïµ, L)
# @timev Î¸, E, n, t = hmc(f_logÏ€, f_gradlogÏ€, M, Mâ»Â¹, Ïµ, L, N);
# @benchmark hmc(f_logÏ€, f_gradlogÏ€, M, Mâ»Â¹, Ïµ, L, N)

# ebfmi(E)

# Epq, Eq = microcanonicalenergy(f_logÏ€, Î¸, E);
# pâ‚• = histogram([Epq Eq] .- [mean(Epq) mean(Eq)], labels=["E_pq â‰¡ Ï€(E|q)" "E_q â‰¡ Ï€(E)"], alpha=0.5,
#                annotations=((0.1, 1.0), text("E-BFMI = $(round(ebfmi(E), digits=3))", 8)),
#                xlabel="E - <E>");
# savefig(pâ‚•, joinpath(pwd(), "energy.pdf"))

# # Multiple chains
# @timev chains = [hmc(f_logÏ€, f_gradlogÏ€, M, Mâ»Â¹, Ïµ, L, N) for _ = 1:4];
# acprob = getindex.(chains, 3) ./ getindex.(chains, 4)
# Î¸ = cat(first.(chains)..., dims=3);
# mean(Î¸, dims=(2,3))
# E = hcat(getindex.(chains, 2)...);

# Ï‘ = originalparam(Î¸);
# mean(Ï‘, dims=(2, 3))
# # Convergence check
# neff(Ï‘)
# Rhat(Ï‘)

# ps = map(1:4) do k
#     Epq, Eq = microcanonicalenergy(f_logÏ€, Î¸[:, :, k], E[:, k])
#     pâ‚• = histogram([Epq Eq] .- [mean(Epq) mean(Eq)], labels=["E_pq â‰¡ Ï€(E|q)" "E_q â‰¡ Ï€(E)"],
#                    alpha=0.5,
#                    annotations=((0.1, 1.0), text("E-BFMI = $(round(ebfmi(E[:, k]), digits=3))", 8)),
#                    xlabel="E - <E>")
#     pâ‚•
# end;
# pâ‚• = plot(ps...);
# savefig(pâ‚•, joinpath(pwd(), "energy_$(N_chains)chain.pdf"))

# #### Centered parameterization -- might get stuck due to small Ï„
# Ïµ = 0.1
# L = 10
# N = 1000
# N_chains = 4

# scale = 15;
# M = diagm(fill(1 / scale^2, 10)); M[10, 10] = 1;
# Mâ»Â¹ = diagm(1 ./ diag(M));

# f_logÏ€_c = let y = y, ÏƒÂ² = ÏƒÂ²
#     q -> logÏ€q(q, y, ÏƒÂ²)
# end;
# f_gradlogÏ€_c = let y = y, ÏƒÂ² = ÏƒÂ²
#     q -> gradlogÏ€(q, y, ÏƒÂ²)
# end;


# qq, pp = transition(f_logÏ€_c, f_gradlogÏ€_c, M, Mâ»Â¹, Ïµ, L)
# # Typically, centered parameterization necessitates a non-small Ï„ starting point
# qáµ¢, páµ¢, táµ¢ = initialize(f_logÏ€_c, f_gradlogÏ€_c, M, Mâ»Â¹, Ïµ, L)
# @timev Î¸, E, n, t = hmc(f_logÏ€_c, f_gradlogÏ€_c, qáµ¢, M, Mâ»Â¹, Ïµ, L, N);


# Epq, Eq = microcanonicalenergy(f_logÏ€_c, Î¸, E);
# pâ‚• = histogram([Epq Eq] .- [mean(Epq) mean(Eq)], labels=["E_pq â‰¡ Ï€(E|q)" "E_q â‰¡ Ï€(E)"], alpha=0.5,
#                annotations=((0.1, 1.0), text("E-BFMI = $(round(ebfmi(E), digits=3))", 8)),
#                xlabel="E - <E>");
# savefig(pâ‚•, joinpath(pwd(), "energy_c.pdf"))

# # Multiple chains
# @timev chains = [hmc(f_logÏ€_c, f_gradlogÏ€_c,
#                      first(initialize(f_logÏ€_c, f_gradlogÏ€_c, M, Mâ»Â¹, Ïµ, L)), # qáµ¢'s
#                      M, Mâ»Â¹, Ïµ, L, N) for _ = 1:N_chains];
# acprob = getindex.(chains, 3) ./ getindex.(chains, N_chains)
# Î¸ = cat(first.(chains)..., dims=3);
# mean(Î¸, dims=(2,3))
# E = hcat(getindex.(chains, 2)...);

# # Convergence check
# neff(Î¸)
# Rhat(Î¸)

# ps = map(1:4) do k
#     Epq, Eq = microcanonicalenergy(f_logÏ€_c, Î¸[:, :, k], E[:, k])
#     pâ‚• = histogram([Epq Eq] .- [mean(Epq) mean(Eq)], labels=["E_pq â‰¡ Ï€(E|q)" "E_q â‰¡ Ï€(E)"],
#                    alpha=0.5,
#                    annotations=((0.1, 1.0), text("E-BFMI = $(round(ebfmi(E[:, k]), digits=3))", 8)),
#                    xlabel="E - <E>")
#     pâ‚•
# end;
# pâ‚• = plot(ps...);
# savefig(pâ‚•, joinpath(pwd(), "energy_c_$(N_chains)chain.pdf"))
# ############################################################################################
# #### MCMC diagnostics: potential scale reduction and effective sample size
# # Simulations arranged on dimension 2, chains on dimension 3
# # function withinchainmean(chain::AbstractMatrix)
# #     ÏˆÌ„ = mean(chain, dims=2)
# # end
# function betweenchain(Ï‘::Array{T, 3}) where {T<:Real}
#     B = size(Ï‘, 2) .* var(dropdims(mean(Ï‘, dims=2), dims=2), dims=2)
# end

# function withinchain(Ï‘::Array{T, 3}) where {T<:Real}
#     W = mean(dropdims(var(Ï‘, dims=2), dims=2), dims=2)
# end

# function vhat(Ïˆ::AbstractMatrix{T}) where {T<:Real}
#     n = size(Ïˆ, 1)
#     B = n * var(mean(Ïˆ, dims=1))
#     W = mean(var(Ïˆ, dims=1))
#     vÌ‚âº = ((n - 1) / n) * W + (1 / n) * B
# end


# function vhat(Ï‘::Array{T, 3}) where {T<:Real}
#     n = size(Ï‘, 2)
#     B = n .* var(dropdims(mean(Ï‘, dims=2), dims=2), dims=2)
#     W = mean(dropdims(var(Ï‘, dims=2), dims=2), dims=2)
#     vÌ‚âº = ((n - 1) / n) .* W .+ (1 / n) .* B
# end

# function Rhat(Ïˆ::AbstractMatrix{T}) where {T<:Real}
#     n = size(Ïˆ, 1)
#     B = n * var(mean(Ïˆ, dims=1))
#     W = mean(var(Ïˆ, dims=1))
#     vÌ‚âº = ((n - 1) / n) * W + (1 / n) * B
#     RÌ‚ = âˆš(vÌ‚âº / W)
# end

# function Rhat(Ï‘::Array{T, 3}) where {T<:Real}
#     n = size(Ï‘, 2)
#     B = n .* var(dropdims(mean(Ï‘, dims=2), dims=2), dims=2)
#     W = mean(dropdims(var(Ï‘, dims=2), dims=2), dims=2)
#     vÌ‚âº = ((n - 1) / n) .* W .+ (1 / n) .* B
#     RÌ‚ = sqrt.(vÌ‚âº ./ W)
# end


# Ï‘ = Î¸[:, (N >> 1 + 1):N, :]; #view(Î¸, :, (N >> 1 + 1):N, :);
# L = size(Ï‘, 2)
# n = L >> 1
# Ï‘â‚‚ = [view(Ï‘, :, 1:n, :);;; view(Ï‘, :, (n + 1):L, :)];

# Ïˆ = Ï‘â‚‚[1, :, :];

# B = betweenchain(Ï‘)
# W = withinchain(Ï‘)
# vÌ‚âº = ((L - 1) / L) .* W .+ (1 / L) .* B
# RÌ‚ = sqrt.(vÌ‚âº ./ W)

# Rhat(Ï‘)
# Rhat(Ï‘â‚‚)

# function variogram(Ïˆ::AbstractMatrix{T}, t::Int) where {T<:Real}
#     n, m = size(Ïˆ)
#     s = zero(T)
#     for j âˆˆ axes(Ïˆ, 2)
#         for i = (t + 1):n
#             s += abs2(Ïˆ[i, j] - Ïˆ[i - t, j])
#         end
#     end
#     return s / (m * (n - t))
# end

# autocor(Ïˆ::AbstractMatrix{T}, t::Int) where {T<:Real} = one(T) - variogram(Ïˆ, t) / 2vhat(Ïˆ)

# function findautolag(Ïˆ::AbstractMatrix{T}) where {T<:Real}
#     n = size(Ïˆ, 1)
#     for t = 1:2:(n - 3)
#         ÏÌ‚â‚œâ‚Šâ‚ = autocor(Ïˆ, t)
#         ÏÌ‚â‚œâ‚Šâ‚‚ = autocor(Ïˆ, t + 1)
#         ÏÌ‚â‚œâ‚Šâ‚ + ÏÌ‚â‚œâ‚Šâ‚‚ < 0 && return t
#     end
#     return n - 3
# end

# function neff(Ïˆ::AbstractMatrix{T}) where {T<:Real}
#     n, m = size(Ïˆ)
#     # t = findautolag(Ïˆ)
#     Î£Ï = zero(T)
#     for t = 1:2:(n - 3)
#         ÏÌ‚â‚œâ‚Šâ‚ = autocor(Ïˆ, t)
#         ÏÌ‚â‚œâ‚Šâ‚‚ = autocor(Ïˆ, t + 1)
#         s = ÏÌ‚â‚œâ‚Šâ‚ + ÏÌ‚â‚œâ‚Šâ‚‚
#         s < 0 && break
#         Î£Ï += s
#         # s < 0 ? break : (Î£Ï += s)
#     end
#     nÌ‚â‚‘ = n * m / (one(T) + 2Î£Ï)
# end

# neff(Ï‘::Array{T, 3}) where {T<:Real} = mapslices(neff, Ï‘, dims=(2, 3))
# neff(Ï‘)
# Rhat(Ï‘)

# variogram(Ïˆ, 1)
# findautolag(Ïˆ)
# @benchmark neff(Ïˆ)
# mapslices(neff, cÌƒ, dims=(2, 3))
# mapslices(Rhat, cÌƒ, dims=(2, 3))

################################################################
# Import Turing and DynamicHMC.
using DynamicHMC, Turing

# Model definition.
@model function gdemo(x, y)
    sÂ² ~ InverseGamma(2, 3)
    m ~ Normal(0, sqrt(sÂ²))
    x ~ Normal(m, sqrt(sÂ²))
    y ~ Normal(m, sqrt(sÂ²))
end

# Pull 2,000 samples using DynamicNUTS.
chn = sample(gdemo(1.5, 2.0), DynamicNUTS(), 2000)

#### The classic 8-schools
@model function sch8(y, Ïƒ)
    Ï„ ~ Uniform(0, 225)
    Î¼ ~ Uniform(-225, 225)
    k = length(y)
    Î± ~ filldist(Normal(Î¼, Ï„), k)
    y .~ Normal.(Î±, Ïƒ)
end
@timev chn2 = sample(sch8(y, Ïƒ), DynamicNUTS(), MCMCThreads(), 1000, 4)
Î¸2 = chn2.value.data
mean(Î¸2, dims=1)
chn2[:Î±]
chn3 = sample(sch8(y, Ïƒ), HMC(Ïµ, L), 1000)

############################################################################################
using LinearAlgebra, Statistics
############################################################################################
#General utilities
function diagdet(M::Matrix{T}) where {T<:Real}
    s = one(T)
    for j âˆˆ axes(M, 1)
        s *= M[j, j]
    end
    s
end

function inversediagdet(Mâ»Â¹::Matrix{T}) where {T<:Real}
    s = one(T)
    for j âˆˆ axes(Mâ»Â¹, 1)
        s *= one(T) / Mâ»Â¹[j, j]
    end
    s
end

function weightedss(p, Mâ»Â¹::Matrix{T}) where {T<:Real}
    s = zero(T)
    for j âˆˆ axes(Mâ»Â¹, 1)
        s += Mâ»Â¹[j, j] * abs2(p[j])
        # or:
        # pâ±¼ = p[j]
        # s += pâ±¼ * pâ±¼ * Mâ»Â¹[j, j]
    end
    s
end

function diagweightedss(x, Î¼, M::Matrix{T}) where {T<:Real}
    s = zero(T)
    for j âˆˆ axes(M, 1)
        s += inv(M[j, j]) * abs2(x[j] - Î¼[j])
    end
    s
end

function logÏ€p(p, Mâ»Â¹)
    # s = length(p) * log(2Ï€)
    # s += log(inversediagdet(Mâ»Â¹))
    # s += weightedss(p, Mâ»Â¹)
    # s *= -0.5
    # s
    # # Or, simpler:
    -0.5 * (length(p) * log(2Ï€) + log(inversediagdet(Mâ»Â¹)) + weightedss(p, Mâ»Â¹))
    # # unnormalized density
    # -0.5weightedss(p, Mâ»Â¹) -0.5log(inversediagdet(Mâ»Â¹))
    # -0.5weightedss(p, Mâ»Â¹)
end

# Using the mass matrix directly
function momentumrand_M(M)
    p = randn(size(M, 1))
    @inbounds for j âˆˆ axes(M, 1)
        p[j] *= âˆš(M[j, j])
    end
    p
end

# Using the lower triangular of the cholesky, AAáµ€ = M
function momentumrand_A(A)
    p = randn(size(A, 1))
    @inbounds for j âˆˆ axes(A, 1)
        p[j] *= A[j, j]
    end
    p
end

# Using the inverse mass matrix directly
function momentumrand_Mâ»Â¹(Mâ»Â¹)
    p = randn(size(Mâ»Â¹, 1))
    @inbounds for j âˆˆ axes(Mâ»Â¹, 1)
        p[j] *= inv(âˆš(Mâ»Â¹[j, j]))
    end
    p
end

function targetrand(Mâ»Â¹::Matrix{T}) where {T<:Real}
    K = size(Mâ»Â¹, 1)
    qâ‚€ = randn(K)
    for k âˆˆ eachindex(qâ‚€)
        qâ‚€[k] *= âˆš(Mâ»Â¹[k, k])
    end
    qâ‚€
end
############################################################################################
# Static HMC implementation:
# This is a high-level implementation, not as efficient as I would normally write.
# Much, so much in-place mutation can be used to minimize memory usage...
################################################################
# Functions to support formal version, other than those provided above
function symplecticintegrate(f_gradlogÏ€, qâ‚€, pâ‚€, Mâ»Â¹, Ïµ, L)
    pâ€² = pâ‚€ + 0.5Ïµ * f_gradlogÏ€(qâ‚€)
    qâ€² = qâ‚€ + Ïµ * Mâ»Â¹ * pâ€²
    for l = 2:L
        pâ€² = pâ€² + Ïµ * f_gradlogÏ€(qâ€²)
        qâ€² = qâ€² + Ïµ * Mâ»Â¹ * pâ€²
    end
    pâ€² = pâ€² + 0.5Ïµ * f_gradlogÏ€(qâ€²)
    qâ€², -pâ€²
end

################ Thinking in terms of the Hamiltonian
H(f_logÏ€q, q, p, Mâ»Â¹) = -f_logÏ€q(q) - logÏ€p(p, Mâ»Â¹)
E_pq(f_logÏ€q, q, E) = E + f_logÏ€q(q)
logÏ€p(f_logÏ€q, q, E) = E_pq(f_logÏ€q, q, E)

function transition(f_logÏ€q, f_gradlogÏ€, qâ‚€, pâ‚€, M, Mâ»Â¹, Ïµ, L)
    qâ€², pâ€² = symplecticintegrate(f_gradlogÏ€, qâ‚€, pâ‚€, Mâ»Â¹, Ïµ, L)
    r = exp(-H(f_logÏ€q, qâ€², pâ€², Mâ»Â¹) + H(f_logÏ€q, qâ‚€, pâ‚€, Mâ»Â¹))
    u = rand()
    u < r ? (qâ€², pâ€²) : (qâ‚€, pâ‚€)
end
transition(f_logÏ€q, f_gradlogÏ€, qâ‚€, M, Mâ»Â¹, Ïµ, L) =
    transition(f_logÏ€q, f_gradlogÏ€, qâ‚€, momentumrand_M(M), M, Mâ»Â¹, Ïµ, L)
transition(f_logÏ€q, f_gradlogÏ€, M, Mâ»Â¹, Ïµ, L) =
    transition(f_logÏ€q, f_gradlogÏ€, targetrand(Mâ»Â¹), momentumrand_M(M), M, Mâ»Â¹, Ïµ, L)

function hmc(f_logÏ€q, f_gradlogÏ€, qâ‚€, M, Mâ»Â¹, Ïµ, L, N)
    Î¸ = Matrix{Float64}(undef, size(M, 1), N)
    E = Vector{Float64}(undef, N)
    n, t = 0, 0
    while n < N
        pâ‚€ = momentumrand_M(M)
        qâ€², pâ€² = symplecticintegrate(f_gradlogÏ€, qâ‚€, pâ‚€, Mâ»Â¹, Ïµ, L)
        Eâ‚€ = H(f_logÏ€q, qâ‚€, pâ‚€, Mâ»Â¹)
        Eâ€² = H(f_logÏ€q, qâ€², pâ€², Mâ»Â¹)
        r = exp(-Eâ€² + Eâ‚€)
        if rand() < r
            n += 1
            Î¸[:, n] = qâ€²
            qâ‚€ = qâ€²
            E[n] = Eâ€²
        end
        t += 1
    end
    Î¸, E, n, t
end
hmc(f_logÏ€q, f_gradlogÏ€, M, Mâ»Â¹, Ïµ, L, N) =
    hmc(f_logÏ€q, f_gradlogÏ€, targetrand(Mâ»Â¹), M, Mâ»Â¹, Ïµ, L, N)

function ebfmi(E::Vector{T}) where {T<:Real}
    N = length(E)
    s = zero(T)
    # As written, this has _clear_ potential for numerical stability issues
    for n = 2:N
        s += abs2(E[n] - E[n - 1])
    end
    s / (N * var(E, corrected=false))
end

function microcanonicalenergy(f_logÏ€q, q, E)
    Eq = -f_logÏ€q(q)
    Epq = E - Epq
    Epq, Eq
end

function microcanonicalenergy(f_logÏ€q, Q::Matrix{T}, E::Vector{T}) where {T<:Real}
    N = length(E)
    Eq = Vector{Float64}(undef, N)
    Epq = Vector{Float64}(undef, N)
    for (n, q) âˆˆ enumerate(eachcol(Q))
        Eq[n] = -f_logÏ€q(q)
        Epq[n] = E[n] - Eq[n]
    end
    Epq, Eq
end

# Guaranteed initialization for models in which small Ï„ causes sticking
# (i.e. for centered parameterizations)
function initialize(f_logÏ€q, f_gradlogÏ€, M, Mâ»Â¹, Ïµ, L)
    t = 0
    while true
        qâ‚€, pâ‚€ = targetrand(Mâ»Â¹), momentumrand_M(M)
        qâ€², pâ€² = transition(f_logÏ€q, f_gradlogÏ€, qâ‚€, pâ‚€, M, Mâ»Â¹, Ïµ, L)
        t += 1
        qâ€² != qâ‚€ && return qâ€², pâ€², t
    end
    return qâ€², pâ€², t
end

############################################################################################
# Centered parameterization:
# yâ±¼ ~ N(Î±â±¼, Ïƒâ±¼Â²)
# Î±â±¼ ~ N(Î¼, Ï„Â²)
# Î¼ ~ U(-âˆ, âˆ)
# Ï„ ~ U(0, âˆ)
# Ï„ transformed to logÏ„ to place on unconstrained space

function gradlogÏ€(Î±, Î¼, logÏ„, y, ÏƒÂ²)
    Ï„Â² = abs2(exp(logÏ„))
    # âˆ‡l = @. - (Î± - y) / ÏƒÂ² - (Î± - Î¼) / Ï„Â²
    âˆ‡l = .- (Î± .- y) ./ ÏƒÂ² .- (Î± .- Î¼) ./ Ï„Â²
    dÏ€dÎ¼ = 0.
        for j âˆˆ axes(Î±, 1)
            dÏ€dÎ¼ -= (Î¼ - Î±[j]) / Ï„Â² # -
        end
    dÏ€dlogÏ„ = - length(Î±) + 1.
        for j âˆˆ axes(Î±, 1)
            dÏ€dlogÏ„ += abs2(Î±[j] - Î¼) / Ï„Â²
        end
    [âˆ‡l; dÏ€dÎ¼; dÏ€dlogÏ„]
end
gradlogÏ€(q, y, ÏƒÂ²) = gradlogÏ€(q[1:8], q[9], q[10], y, ÏƒÂ²)

function logÏ€q(Î±, Î¼, logÏ„, y, ÏƒÂ²)
    Ï„Â² = abs2(exp(logÏ„))
    # s = 0
    # s += logÏ„
    s = logÏ„
    for j âˆˆ axes(Î±, 1)
        s += -log(âˆš(2Ï€ * ÏƒÂ²[j])) - 0.5abs2(y[j] - Î±[j]) / ÏƒÂ²[j] - 0.5abs2(Î±[j] - Î¼) / Ï„Â²
        # s += - 0.5log(2Ï€ * ÏƒÂ²[j]) - 0.5abs2(y[j] - Î±[j]) / ÏƒÂ²[j] - 0.5abs2(Î±[j] - Î¼) / Ï„Â²
    end
    s -= length(Î±) * log(âˆš(2Ï€ * Ï„Â²))
    # s -= length(Î±) * 0.5log(2Ï€ * Ï„Â²)
    s
    # # unnormalized density
    # Ï„Â² = abs2(exp(logÏ„))
    # s = logÏ„
    # for j âˆˆ axes(Î±, 1)
    #     s += -0.5abs2(y[j] - Î±[j]) / ÏƒÂ²[j] - 0.5abs2(Î±[j] - Î¼) / Ï„Â²
    # end
    # s += -0.5log(2Ï€ * Ï„Â²)
    # s
end
logÏ€q(q, y, ÏƒÂ²) = logÏ€q(q[1:8], q[9], q[10], y, ÏƒÂ²)

# A faster non-allocating version to replace the simple convenience above:
function logÏ–q(q, y, ÏƒÂ²)
    @inbounds Î¼ = q[9]
    @inbounds logÏ„ = q[10]
    Ï„Â² = abs2(exp(logÏ„))
    s = logÏ„
    @inbounds @simd for j âˆˆ eachindex(y, ÏƒÂ²)
        # s += -0.5log(2Ï€ * ÏƒÂ²[j]) - 0.5abs2(y[j] - q[j]) / ÏƒÂ²[j] - 0.5abs2(q[j] - Î¼) / Ï„Â²
        s += -0.5(log(2Ï€ * ÏƒÂ²[j]) + abs2(y[j] - q[j]) / ÏƒÂ²[j] + abs2(q[j] - Î¼) / Ï„Â²)
    end
    s += -0.5 * length(y) * log(2Ï€ * Ï„Â²)
    s
end


############################################################################################
#### 2022-02-14: non-centered parameterization
# yâ±¼ ~ N(Î¼ + Ï„ * Î·â±¼, Ïƒâ±¼Â²)
# Î·â±¼ ~ N(0, 1)
# Î¼ ~ U(-âˆ, âˆ)
# Ï„ ~ U(0, âˆ)
# Ï„ transformed to logÏ„ to place on unconstrained space

# Clearly not the most efficient approach -- the proper way is to compute a vector, the
# elements of which are (y[j] - Î¼ - Ï„ * Î·[j]). Then, compute dÏ€dÎ¼ and dÏ€dlogÏ„, then
# mutate in place to complete the computation of âˆ‡l.
function gradlogÏ€_nc(Î·, Î¼, logÏ„, y, ÏƒÂ²)
    Ï„ = exp(logÏ„)
    âˆ‡l = Vector{Float64}(undef, length(Î·))
    for j âˆˆ eachindex(Î·, y, ÏƒÂ²)
        âˆ‡l[j] = (y[j] - Î¼ - Ï„ * Î·[j]) * Ï„ / ÏƒÂ²[j] - Î·[j]
    end
    dÏ€dÎ¼ = 0.0
    for j âˆˆ eachindex(Î·, y, ÏƒÂ²)
        dÏ€dÎ¼ += (y[j] - Î¼ - Ï„ * Î·[j]) / ÏƒÂ²[j]
    end
    dÏ€dlogÏ„ = 1.0
    for j âˆˆ eachindex(Î·, y, ÏƒÂ²)
        dÏ€dlogÏ„ += Ï„ * (y[j] - Î¼ - Ï„ * Î·[j]) * Î·[j] / ÏƒÂ²[j]
    end
    [âˆ‡l; dÏ€dÎ¼; dÏ€dlogÏ„]
end
gradlogÏ€_nc(q, y, ÏƒÂ²) = gradlogÏ€_nc(q[1:8], q[9], q[10], y, ÏƒÂ²)

function logÏ€q_nc(Î·, Î¼, logÏ„, y, ÏƒÂ²)
    Ï„ = exp(logÏ„)
    s = logÏ„
    for j âˆˆ eachindex(Î·, y, ÏƒÂ²)
        s += -0.5(log(2Ï€ * ÏƒÂ²[j]) + (1.0 / ÏƒÂ²[j]) * abs2(y[j] - Î¼ - Ï„ * Î·[j]) + log(2Ï€) + abs2(Î·[j]))
    end
    s
end
logÏ€q_nc(q, y, ÏƒÂ²) = logÏ€q_nc(q[1:8], q[9], q[10], y, ÏƒÂ²)

# A faster non-allocating version to replace the simple convenience above:
function gradlogÏ–_nc(q::Vector{T}, y, ÏƒÂ²) where {T<:Real}
    @inbounds Î¼ = q[9]
    @inbounds logÏ„ = q[10]
    Ï„ = exp(logÏ„)
    âˆ‡l = Vector{T}(undef, length(q))
    # for j âˆˆ eachindex(y, ÏƒÂ²)
    #     âˆ‡l[j] = (y[j] - Î¼ - Ï„ * q[j]) * Ï„ / ÏƒÂ²[j] - q[j]
    # end
    # dÏ€dÎ¼ = 0.0
    # for j âˆˆ eachindex(y, ÏƒÂ²)
    #     dÏ€dÎ¼ += (y[j] - Î¼ - Ï„ * q[j]) / ÏƒÂ²[j]
    # end
    # âˆ‡l[9] = dÏ€dÎ¼
    # dÏ€dlogÏ„ = 1.0
    # for j âˆˆ eachindex(y, ÏƒÂ²)
    #     dÏ€dlogÏ„ += Ï„ * (y[j] - Î¼ - Ï„ * q[j]) * q[j] / ÏƒÂ²[j]
    # end
    # âˆ‡l[10] = dÏ€dlogÏ„
    # âˆ‡l
    # or, even better
    dÏ€dÎ¼ = zero(T)
    dÏ€dlogÏ„ = one(T)
    @inbounds @simd for j âˆˆ eachindex(y, ÏƒÂ²)
        # yâ±¼ = y[j]
        # Î·â±¼ = q[j]
        # Ïƒâ±¼Â² = ÏƒÂ²[j]
        # Î” = (yâ±¼ - Î¼ - Ï„ * Î·â±¼)
        # âˆ‡l[j] = Î” * Ï„ / Ïƒâ±¼Â² - Î·â±¼
        # dÏ€dÎ¼ += Î” / Ïƒâ±¼Â²
        # dÏ€dlogÏ„ += Ï„ * Î” * Î·â±¼ / Ïƒâ±¼Â²
        yâ±¼ = y[j]
        Î·â±¼ = q[j]
        iÏƒâ±¼Â² = inv(ÏƒÂ²[j])
        Î” = yâ±¼ - Î¼ - Ï„ * Î·â±¼
        Î”iÏƒâ±¼Â² = Î” * iÏƒâ±¼Â²
        âˆ‡l[j] = Ï„ * Î”iÏƒâ±¼Â² - Î·â±¼
        dÏ€dÎ¼ += Î”iÏƒâ±¼Â²
        dÏ€dlogÏ„ += Ï„ * Î”iÏƒâ±¼Â² * Î·â±¼
    end
    âˆ‡l[9] = dÏ€dÎ¼
    âˆ‡l[10] = dÏ€dlogÏ„
    âˆ‡l
end

function logÏ–q_nc(q, y, ÏƒÂ²)
    @inbounds Î¼ = q[9]
    @inbounds logÏ„ = q[10]
    Ï„ = exp(logÏ„)
    s = logÏ„
    # @turbo for j âˆˆ eachindex(y, ÏƒÂ²)
    #     s += -0.5(log(2Ï€ * ÏƒÂ²[j]) + (1.0 / ÏƒÂ²[j]) * abs2(y[j] - Î¼ - Ï„ * q[j]) + log(2Ï€) + abs2(q[j]))
    # end
    @inbounds @simd for j âˆˆ eachindex(y, ÏƒÂ²)
        Î·â±¼ = q[j]
        Ïƒâ±¼Â² = ÏƒÂ²[j]
        s += -0.5(log(2Ï€ * Ïƒâ±¼Â²) + abs2(y[j] - Î¼ - Ï„ * Î·â±¼) / Ïƒâ±¼Â² + log(2Ï€) + abs2(Î·â±¼))
    end
    s
end

#### Utilities for conversion to original parameterization
function originalparam(Î¸::Vector{T}) where {T<:Real}
    K = length(Î¸)
    Ï„ = exp(Î¸[K])
    Î¼ = Î¸[K - 1]
    Ï‘ = Vector{promote_type(T, Float64)}(undef, K)
    for k = 1:(K - 2)
        Ï‘[k] = Î¼ + Ï„ * Î¸[k]
    end
    Ï‘[K - 1] = Î¼
    Ï‘[K] = Ï„
    Ï‘
end

function originalparam(Î¸::Matrix{T}) where {T<:Real}
    K, S = size(Î¸)
    Ï‘ = Matrix{promote_type(T, Float64)}(undef, K, S)
    for s âˆˆ axes(Î¸, 2)
        Ï„ = exp(Î¸[K, s])
        Î¼ = Î¸[K - 1, s]
        for k = 1:(K - 2)
            Ï‘[k, s] = Î¼ + Ï„ * Î¸[k, s]
        end
        Ï‘[K - 1, s] = Î¼
        Ï‘[K, s] = Ï„
    end
    Ï‘
end
originalparam(Î¸::Array{T, 3}) where {T<:Real} = mapslices(originalparam, Î¸, dims=(1, 2))

############################################################################################
#### MCMC diagnostics: potential scale reduction and effective sample size
# Simulations arranged on dimension 2, chains on dimension 3
# function withinchainmean(chain::AbstractMatrix)
#     ÏˆÌ„ = mean(chain, dims=2)
# end
function betweenchain(Ï‘::Array{T, 3}) where {T<:Real}
    B = size(Ï‘, 2) .* var(dropdims(mean(Ï‘, dims=2), dims=2), dims=2)
end

function withinchain(Ï‘::Array{T, 3}) where {T<:Real}
    W = mean(dropdims(var(Ï‘, dims=2), dims=2), dims=2)
end

function vhat(Ïˆ::AbstractMatrix{T}) where {T<:Real}
    n = size(Ïˆ, 1)
    B = n * var(mean(Ïˆ, dims=1))
    W = mean(var(Ïˆ, dims=1))
    vÌ‚âº = ((n - 1) / n) * W + (1 / n) * B
end


function vhat(Ï‘::Array{T, 3}) where {T<:Real}
    n = size(Ï‘, 2)
    B = n .* var(dropdims(mean(Ï‘, dims=2), dims=2), dims=2)
    W = mean(dropdims(var(Ï‘, dims=2), dims=2), dims=2)
    vÌ‚âº = ((n - 1) / n) .* W .+ (1 / n) .* B
end

function Rhat(Ïˆ::AbstractMatrix{T}) where {T<:Real}
    n = size(Ïˆ, 1)
    B = n * var(mean(Ïˆ, dims=1))
    W = mean(var(Ïˆ, dims=1))
    vÌ‚âº = ((n - 1) / n) * W + (1 / n) * B
    RÌ‚ = âˆš(vÌ‚âº / W)
end

function Rhat(Ï‘::Array{T, 3}) where {T<:Real}
    n = size(Ï‘, 2)
    B = n .* var(dropdims(mean(Ï‘, dims=2), dims=2), dims=2)
    W = mean(dropdims(var(Ï‘, dims=2), dims=2), dims=2)
    vÌ‚âº = ((n - 1) / n) .* W .+ (1 / n) .* B
    RÌ‚ = sqrt.(vÌ‚âº ./ W)
end

function variogram(Ïˆ::AbstractMatrix{T}, t::Int) where {T<:Real}
    n, m = size(Ïˆ)
    s = zero(T)
    for j âˆˆ axes(Ïˆ, 2)
        for i = (t + 1):n
            s += abs2(Ïˆ[i, j] - Ïˆ[i - t, j])
        end
    end
    return s / (m * (n - t))
end

autocor(Ïˆ::AbstractMatrix{T}, t::Int) where {T<:Real} = one(T) - variogram(Ïˆ, t) / 2vhat(Ïˆ)

function findautolag(Ïˆ::AbstractMatrix{T}) where {T<:Real}
    n = size(Ïˆ, 1)
    for t = 1:2:(n - 3)
        ÏÌ‚â‚œâ‚Šâ‚ = autocor(Ïˆ, t)
        ÏÌ‚â‚œâ‚Šâ‚‚ = autocor(Ïˆ, t + 1)
        ÏÌ‚â‚œâ‚Šâ‚ + ÏÌ‚â‚œâ‚Šâ‚‚ < 0 && return t
    end
    return n - 3
end

function neff(Ïˆ::AbstractMatrix{T}) where {T<:Real}
    n, m = size(Ïˆ)
    # t = findautolag(Ïˆ)
    Î£Ï = zero(T)
    for t = 1:2:(n - 3)
        ÏÌ‚â‚œâ‚Šâ‚ = autocor(Ïˆ, t)
        ÏÌ‚â‚œâ‚Šâ‚‚ = autocor(Ïˆ, t + 1)
        s = ÏÌ‚â‚œâ‚Šâ‚ + ÏÌ‚â‚œâ‚Šâ‚‚
        s < 0 && break
        Î£Ï += s
        # s < 0 ? break : (Î£Ï += s)
    end
    nÌ‚â‚‘ = n * m / (one(T) + 2Î£Ï)
end

neff(Ï‘::Array{T, 3}) where {T<:Real} = mapslices(neff, Ï‘, dims=(2, 3))

############################################################################################
using Plots
gr(size=(1200,800))
# Example for non-centered parameterization
# Data
y = Float64[28, 8, -3, 7, -1, 1, 18, 12];
Ïƒ = Float64[15, 10, 16, 11, 9, 11, 10, 18];
ÏƒÂ² = abs2.(Ïƒ);

# Integration time and stepsize. These are suggested values -- try some others, keeping ÏµL=1
Ïµ = 0.2
L = 5
N = 1000
N_chains = 8

# Euclidean-Gaussian metric
scale = 15;
Mâ»Â¹ = diagm(ones(10)); Mâ»Â¹[9, 9] = scale^2;
M = diagm(1 ./ diag(Mâ»Â¹));

f_logÏ€ = let y = y, ÏƒÂ² = ÏƒÂ²
    q -> logÏ€q_nc(q, y, ÏƒÂ²)
end;
f_gradlogÏ€ = let y = y, ÏƒÂ² = ÏƒÂ²
    q -> gradlogÏ€_nc(q, y, ÏƒÂ²)
end;
f_logÏ– = let y = y, ÏƒÂ² = ÏƒÂ²
    q -> logÏ–q_nc(q, y, ÏƒÂ²)
end;
f_gradlogÏ– = let y = y, ÏƒÂ² = ÏƒÂ²
    q -> gradlogÏ–_nc(q, y, ÏƒÂ²)
end;

@timev Î¸, E, n, t = hmc(f_logÏ€, f_gradlogÏ€, M, Mâ»Â¹, Ïµ, L, N);
@timev Î¸, E, n, t = hmc(f_logÏ–, f_gradlogÏ–, M, Mâ»Â¹, Ïµ, L, N);
Ï‘ = originalparam(Î¸);
mean(Ï‘, dims=2)

ebfmi(E)

Epq, Eq = microcanonicalenergy(f_logÏ€, Î¸, E);
pâ‚• = histogram([Epq Eq] .- [mean(Epq) mean(Eq)], labels=["E_pq â‰¡ Ï€(E|q)" "E_q â‰¡ Ï€(E)"], alpha=0.5,
               annotations=((0.1, 1.0), text("E-BFMI = $(round(ebfmi(E), digits=3))", 8)),
               xlabel="E - <E>");
savefig(pâ‚•, joinpath(pwd(), "energy.pdf"))

# Multiple chains
@timev chains = [hmc(f_logÏ€, f_gradlogÏ€, M, Mâ»Â¹, Ïµ, L, N) for _ = 1:N_chains];
acprob = getindex.(chains, 3) ./ getindex.(chains, 4)
Î¸ = cat(first.(chains)..., dims=3);
mean(Î¸, dims=(2,3))
E = hcat(getindex.(chains, 2)...);

Ï‘â‚€ = originalparam(Î¸);
Ï‘ = originalparam(Î¸[:, (N >> 1 + 1):N, :]);
mean(Ï‘, dims=(2, 3))
# Convergence check
nâ‚‚ = size(Ï‘, 2)
n = nâ‚‚ >> 1
Ï‘â‚‚ = [view(Ï‘, :, 1:n, :);;; view(Ï‘, :, (n + 1):nâ‚‚, :)];
neff(Ï‘â‚‚)
Rhat(Ï‘â‚‚)

ps = map(1:N_chains) do k
    Epq, Eq = microcanonicalenergy(f_logÏ€, Î¸[:, :, k], E[:, k])
    pâ‚• = histogram([Epq Eq] .- [mean(Epq) mean(Eq)], labels=["E_pq â‰¡ Ï€(E|q)" "E_q â‰¡ Ï€(E)"],
                   alpha=0.5,
                   annotations=((0.1, 1.0), text("E-BFMI = $(round(ebfmi(E[:, k]), digits=3))", 8)),
                   xlabel="E - <E>")
    pâ‚•
end;
pâ‚• = plot(ps...);
savefig(pâ‚•, joinpath(pwd(), "energy_$(N_chains)chain.pdf"))

# Neat trace plots
ps = map(1:N_chains) do k
    plot(permutedims(Ï‘â‚€[:, :, k]), labels=["Î¸$(tosubscript(i))" for _ = 1:1, i = 1:10])
end;
p = plot(ps..., link=:all);
savefig(p, joinpath(pwd(), "hmc_$(N_chains)chains_trace.pdf"))

# autocorrelation plots
Ï‘â±¼ = Ï‘[1, :];
acf = map(0:500) do k
    acor(Ï‘â±¼, k)
end;
p = bar(acf, label="empirical")
savefig(p, joinpath(pwd(), "hmc_autocorrelation.pdf"))
#
ps = map(1:size(Ï‘, 1)) do j
    Ï‘â±¼ = Ï‘[j, :]
    acf = map(0:500) do k
        acor(Ï‘â±¼, k)
    end;
    p = bar(acf, label="Î¸$(j)")
    p
end;
p = plot(ps...);
savefig(p, joinpath(pwd(), "hmc_autocorrelation.pdf"))

#################### Centered parameterization -- might get stuck due to small Ï„
Ïµ = 0.1
L = 10
N = 1000
N_chains = 8

scale = 15;
M = diagm(fill(1 / scale^2, 10)); M[10, 10] = 1;
Mâ»Â¹ = diagm(1 ./ diag(M));

f_logÏ€_c = let y = y, ÏƒÂ² = ÏƒÂ²
    q -> logÏ€q(q, y, ÏƒÂ²)
end;
f_gradlogÏ€_c = let y = y, ÏƒÂ² = ÏƒÂ²
    q -> gradlogÏ€(q, y, ÏƒÂ²)
end;


# Typically, centered parameterization necessitates a non-small Ï„ starting point, so guarantee it
qáµ¢, páµ¢, táµ¢ = initialize(f_logÏ€_c, f_gradlogÏ€_c, M, Mâ»Â¹, Ïµ, L)
@timev Î¸, E, n, t = hmc(f_logÏ€_c, f_gradlogÏ€_c, qáµ¢, M, Mâ»Â¹, Ïµ, L, N);

ebfmi(E)

Epq, Eq = microcanonicalenergy(f_logÏ€_c, Î¸, E);
pâ‚• = histogram([Epq Eq] .- [mean(Epq) mean(Eq)], labels=["E_pq â‰¡ Ï€(E|q)" "E_q â‰¡ Ï€(E)"], alpha=0.5,
               annotations=((0.1, 1.0), text("E-BFMI = $(round(ebfmi(E), digits=3))", 8)),
               xlabel="E - <E>");
savefig(pâ‚•, joinpath(pwd(), "energy_c.pdf"))

# Multiple chains
@timev chains = [hmc(f_logÏ€_c, f_gradlogÏ€_c,
                     first(initialize(f_logÏ€_c, f_gradlogÏ€_c, M, Mâ»Â¹, Ïµ, L)), # qáµ¢'s
                     M, Mâ»Â¹, Ïµ, L, N) for _ = 1:N_chains];
acprob = getindex.(chains, 3) ./ getindex.(chains, 4)
Î¸ = cat(first.(chains)..., dims=3);
mean(Î¸, dims=(2,3))
E = hcat(getindex.(chains, 2)...);

# Convergence check
neff(Î¸)
Rhat(Î¸)

Ï‘ = Î¸[:, (N >> 1 + 1):N, :];
mean(Ï‘, dims=(2, 3))
# Convergence check
nâ‚‚ = size(Ï‘, 2)
n = nâ‚‚ >> 1
Ï‘â‚‚ = [view(Ï‘, :, 1:n, :);;; view(Ï‘, :, (n + 1):nâ‚‚, :)];
neff(Ï‘â‚‚)
Rhat(Ï‘â‚‚)

ps = map(1:N_chains) do k
    Epq, Eq = microcanonicalenergy(f_logÏ€_c, Î¸[:, :, k], E[:, k])
    pâ‚• = histogram([Epq Eq] .- [mean(Epq) mean(Eq)], labels=["E_pq â‰¡ Ï€(E|q)" "E_q â‰¡ Ï€(E)"],
                   alpha=0.5,
                   annotations=((0.1, 1.0), text("E-BFMI = $(round(ebfmi(E[:, k]), digits=3))", 8)),
                   xlabel="E - <E>")
    pâ‚•
end;
pâ‚• = plot(ps...);
savefig(pâ‚•, joinpath(pwd(), "energy_c_$(N_chains)chain.pdf"))


# Neat trace plots
ps = map(1:N_chains) do k
    plot(permutedims(Î¸[:, :, k]), labels=["Î¸$(tosubscript(i))" for _ = 1:1, i = 1:10])
end;
p = plot(ps..., link=:all);
savefig(p, joinpath(pwd(), "hmc_c_$(N_chains)chains_trace.pdf"))

# autocorrelation plots
Î¸â±¼ = Î¸[1, :];
acf = map(0:500) do k
    acor(Î¸â±¼, k)
end;
p = bar(acf, label="empirical")
savefig(p, joinpath(pwd(), "hmc_c_autocorrelation.pdf"))
#
ps = map(1:size(Î¸, 1)) do j
    Î¸â±¼ = Î¸[j, :]
    acf = map(0:500) do k
        acor(Î¸â±¼, k)
    end;
    p = bar(acf, label="Î¸$(j)")
    p
end;
p = plot(ps...);
savefig(p, joinpath(pwd(), "hmc_c_autocorrelation.pdf"))

############################################################################################
#### 2022-02-16: Gibbs sampler, centered parameterization
using Distributions

function sample_Î±(Î¼, Ï„Â², y, ÏƒÂ²)
    Î± = randn(length(y))
    iÏ„Â² = inv(Ï„Â²)
    Î¼iÏ„Â² = Î¼ / Ï„Â²
    for j âˆˆ eachindex(y, ÏƒÂ²)
        iÏƒâ±¼Â² = inv(ÏƒÂ²[j])
        d = iÏƒâ±¼Â² + iÏ„Â²
        Î±Ì‚â±¼ = (y[j] * iÏƒâ±¼Â² + Î¼iÏ„Â²) / d
        Vâ±¼ = inv(d)
        Î±[j] = Î±Ì‚â±¼ + Î±[j] * âˆšVâ±¼
    end
    Î±
end

sample_Î¼(Î±, Ï„Â²) = mean(Î±) + randn() * âˆš(Ï„Â² / length(Î±))

function sample_Ï„Â²(Î±, Î¼)
    Ï„Ì‚Â² = 0.0
    for j âˆˆ eachindex(Î±)
        Ï„Ì‚Â² += abs2(Î±[j] - Î¼)
    end
    Î½ = length(Î±) - 1
    # # Unnecessary since it is immediately canceled, but worthwhile to show:
    # Ï„Ì‚Â² /= Î½ # compltes Ï„Ì‚Â² computation
    # Ï„Â² = Î½ * Ï„Ì‚Â² / rand(Chisq(Î½))
    Ï„Ì‚Â² / rand(Chisq(Î½)) # Ï„Ì‚Â² / rand(Gamma(Î½ / 2, 2.0)) is slightly faster
end

function gibbs(Î±â‚€, Î¼â‚€, y, ÏƒÂ², N)
    Î¸ = Matrix{Float64}(undef, 10, N)
    Î± = Î±â‚€
    Î¼ = Î¼â‚€
    for n = 1:N
        Ï„Â² = sample_Ï„Â²(Î±, Î¼)
        Î¼ = sample_Î¼(Î±, Ï„Â²)
        Î± = sample_Î±(Î¼, Ï„Â², y, ÏƒÂ²)
        Î¸[1:8, n] = Î±
        Î¸[9, n] = Î¼
        Î¸[10, n] = Ï„Â²
    end
    Î¸
end

# Experiment with different initial points
Î±â‚€ = randn(8) * 15; # Î±â‚€ ~ N(0, 15Â²Iâ‚ˆ)
# Î±â‚€ = randn(8);
# Î±â‚€ = y;
Î¼â‚€ = mean(Î±â‚€) + randn();

N = 1000
N_chains = 4

@timev Î¸ = gibbs(Î±â‚€, Î¼â‚€, y, ÏƒÂ², N);
mean(Î¸[:, (N >> 1 + 1):N], dims=2)
mean(sqrt.(Î¸[10, :]))

p = plot(permutedims(Î¸), labels=["Î¸$(tosubscript(i))" for _ = 1:1, i = 1:10]);
savefig(p, joinpath(pwd(), "trace.pdf"))
p = plot(Î¸[9, :], Î¸[10, :], 1:N, zcolor=(1:N), m = (10, 0.8, :blues, Plots.stroke(0)),
         leg = false, cbar = true);
savefig(p, joinpath(pwd(), "3d.pdf"))

# Multiple chains
@timev chains = [gibbs(Î±â‚€, Î¼â‚€, y, ÏƒÂ², N) for _ = 1:N_chains];
Î¸ = cat(chains..., dims=3);
mean(Î¸, dims=(2,3))
mean(sqrt.(Î¸[10, :, :]))

# Convergence check
neff(Î¸)
Rhat(Î¸)

Ï‘ = Î¸[:, (N >> 1 + 1):N, :];
mean(Ï‘, dims=(2, 3))
# Convergence check
nâ‚‚ = size(Ï‘, 2)
n = nâ‚‚ >> 1
Ï‘â‚‚ = [view(Ï‘, :, 1:n, :);;; view(Ï‘, :, (n + 1):nâ‚‚, :)];
neff(Ï‘â‚‚)
Rhat(Ï‘â‚‚)

# Neat trace plots
ps = map(1:N_chains) do k
    plot(permutedims(Î¸[:, :, k]), labels=["Î¸$(tosubscript(i))" for _ = 1:1, i = 1:10])
end;
p = plot(ps..., link=:all);
savefig(p, joinpath(pwd(), "gibbs_$(N_chains)chains_trace.pdf"))

# Complimentary to the above, this plot enables one to visualize when the Gibbs sampler
# (under the centered parameterization) get stuck at small values of Ï„. The
# effect is quite dramatic!
ps = map(1:N_chains) do k
    Î¸â‚– = Î¸[:, :, k]
    Î¸â‚–[10, :] .= sqrt.(Î¸â‚–[10, :])
    plot(permutedims(Î¸â‚–), labels=["Î¸$(tosubscript(i))" for _ = 1:1, i = 1:10])
end;
p = plot(ps..., link=:all);
savefig(p, joinpath(pwd(), "gibbs_$(N_chains)chains_trace_sqrttau.pdf"))

# autocorrelation plots
Î¸â±¼ = Î¸[1, :];
acf = map(0:500) do k
    acor(Î¸â±¼, k)
end;
p = bar(acf, label="empirical")
savefig(p, joinpath(pwd(), "gibbs_autocorrelation.pdf"))
#
ps = map(1:size(Î¸, 1)) do j
    Î¸â±¼ = Î¸[j, :]
    acf = map(0:500) do k
        acor(Î¸â±¼, k)
    end;
    p = bar(acf, label="Î¸$(j)")
    p
end;
p = plot(ps...);
savefig(p, joinpath(pwd(), "gibbs_autocorrelation.pdf"))

############################################################################################
#### 2022-02-16: Metropolis-Hastings, centered parameterization, d-dimensional vector-jumping

function randmvndiag(Î¼, Î£)
    z = randn(length(Î¼))
    for j âˆˆ eachindex(Î¼, z)
        z[j] = Î¼[j] + z[j] * âˆš(Î£[j, j])
    end
    z
end
function randmvndiag(Î£)
    z = randn(size(Î£, 1))
    @inbounds for j âˆˆ axes(Î£, 1)
        z[j] *= âˆš(Î£[j, j])
    end
    z
end

function logp_mvndiag(x, Î¼, Î£)
    -0.5(length(x) * log(2Ï€) + log(diagdet(Î£)) + diagweightedss(x, Î¼, Î£))
end

function metropolishastings(f_logÏ€q, f_logQ, Î£, N)
    Î¸ = Matrix{Float64}(undef, size(Î£, 1), N)
    n, t = 1, 0
    qâ‚€ = randmvndiag(Î£)
    Î¸[:, n] = qâ‚€
    while n < N
        qâ€² = randmvndiag(qâ‚€, Î£)
        r = exp(f_logÏ€q(qâ€²) + f_logQ(qâ‚€, qâ€², Î£) - f_logÏ€q(qâ‚€) - f_logQ(qâ€², qâ‚€, Î£))
        if rand() < r
            n += 1
            qâ‚€ = qâ€²
            Î¸[:, n] = qâ€²
        end
        t += 1
    end
    Î¸, n, t
end

f_logÏ€q = let y = y, ÏƒÂ² = ÏƒÂ²
    q -> logÏ€q(q, y, ÏƒÂ²)
end;

N = 5000
N_chains = 4

# It is instructive to set the scale to values 0.5, 1.0, 5.0, 15.0, etc.
# and check convergence.
scale = .5;
c = 2.4 / âˆš(10)
Î£ = c^2 * diagm(fill(scale^2, 10)); Î£[10, 10] = 1;

@timev Î¸, n, t = metropolishastings(f_logÏ€q, logp_mvndiag, Î£, N);
mean(Î¸[:, (N >> 1 + 1):N], dims=2)

p = plot(permutedims(Î¸), labels=["Î¸$(tosubscript(i))" for _ = 1:1, i = 1:10]);
savefig(p, joinpath(pwd(), "trace.pdf"))
p = plot(Î¸[9, :], Î¸[10, :], 1:N, zcolor=(1:N), m = (10, 0.8, :blues, Plots.stroke(0)),
         leg = false, cbar = true);
savefig(p, joinpath(pwd(), "3d.pdf"))

# Multiple chains
@timev chains = [metropolishastings(f_logÏ€q, logp_mvndiag, Î£, N) for _ = 1:N_chains];
Î¸ = cat(first.(chains)..., dims=3);
acprob = getindex.(chains, 2) ./ getindex.(chains, 3)
mean(Î¸, dims=(2,3))
mean(exp.(Î¸[10, :, :]))

# Convergence check
neff(Î¸)
Rhat(Î¸)

Ï‘ = Î¸[:, (N >> 1 + 1):N, :];
mean(Ï‘, dims=(2, 3))
# Convergence check
nâ‚‚ = size(Ï‘, 2)
n = nâ‚‚ >> 1
Ï‘â‚‚ = [view(Ï‘, :, 1:n, :);;; view(Ï‘, :, (n + 1):nâ‚‚, :)];
neff(Ï‘â‚‚)
Rhat(Ï‘â‚‚)

# Neat trace plots
ps = map(1:N_chains) do k
    plot(permutedims(Î¸[:, :, k]), labels=["Î¸$(tosubscript(i))" for _ = 1:1, i = 1:10])
end;
p = plot(ps..., link=:all);
savefig(p, joinpath(pwd(), "mh_$(N_chains)chains_trace.pdf"))

# autocorrelation plots
Î¸â±¼ = Î¸[1, :];
acf = map(0:500) do k
    acor(Î¸â±¼, k)
end;
p = bar(acf, label="empirical")
savefig(p, joinpath(pwd(), "mh_autocorrelation.pdf"))
#
ps = map(1:size(Î¸, 1)) do j
    Î¸â±¼ = Î¸[j, :]
    acf = map(0:500) do k
        acor(Î¸â±¼, k)
    end;
    p = bar(acf, label="Î¸$(j)")
    p
end;
p = plot(ps...);
savefig(p, joinpath(pwd(), "mh_autocorrelation.pdf"))

############################################################################################
#### 2022-02-17: Efficient static HMC
function etransition(f_logÏ€q, f_gradlogÏ€, qâ‚€, pâ‚€, M, Mâ»Â¹, wâ‚€, Ïµ, T, L)
    if Ïµ * L â‰¥ T
        return qâ‚€, pâ‚€
    else
        Ïµ = rand(Bool) ? -Ïµ : Ïµ
        wâ€² = 0.0
        pâ€² = pâ‚€ + 0.5Ïµ * f_gradlogÏ€(qâ‚€)
        qâ€² = qâ‚€ + Ïµ * Mâ»Â¹ * pâ€²
        for l = 2:L
            pâ€² = pâ€² + Ïµ * f_gradlogÏ€(qâ€²)
            wâ€² += exp(-H(f_logÏ€q, qâ€², pâ€², Mâ»Â¹))
            qâ€² = qâ€² + Ïµ * Mâ»Â¹ * pâ€²
        end
        pâ€² = pâ€² + 0.5Ïµ * f_gradlogÏ€(qâ€²)
        wâ€² += exp(-H(f_logÏ€q, qâ€², pâ€², Mâ»Â¹))
        ğ“…â‚€ = wâ‚€ / (wâ‚€ + wâ€²)
        if rand() â‰¤ ğ“…â‚€
            # return etransition(f_logÏ€q, f_gradlogÏ€, qâ‚€, rand(Bool) ? -pâ‚€ : pâ‚€,
            #                    M, Mâ»Â¹, wâ‚€ + wâ€², Ïµ, T, 2L)
            return etransition(f_logÏ€q, f_gradlogÏ€, qâ‚€, pâ‚€, M, Mâ»Â¹, wâ‚€ + wâ€², abs(Ïµ), T, 2L)
            # return etransition(f_logÏ€q, f_gradlogÏ€, qâ‚€, pâ‚€, M, Mâ»Â¹, wâ‚€ + wâ€², T, 2L
            #                    rand(Bool) ? -Ïµ : Ïµ, T, 2L)
        else
            # return etransition(f_logÏ€q, f_gradlogÏ€, qâ€², rand(Bool) ? -pâ€² : pâ€²,
            #                    M, Mâ»Â¹, wâ‚€ + wâ€², Ïµ, T, 2L)
            return etransition(f_logÏ€q, f_gradlogÏ€, qâ€², pâ€², M, Mâ»Â¹, wâ‚€ + wâ€², abs(Ïµ), T, 2L)
            # return etransition(f_logÏ€q, f_gradlogÏ€, qâ€², pâ€², M, Mâ»Â¹, wâ‚€ + wâ€²,
            #                    rand(Bool) ? -Ïµ : Ïµ, T, 2L)
        end
    end
end
function etransition(f_logÏ€q, f_gradlogÏ€, qâ‚€, M, Mâ»Â¹, Ïµ, T)
    pâ‚€ = momentumrand_M(M)
    wâ‚€ = exp(-H(f_logÏ€q, qâ‚€, pâ‚€, Mâ»Â¹))
    etransition(f_logÏ€q, f_gradlogÏ€, qâ‚€, pâ‚€, M, Mâ»Â¹, wâ‚€, Ïµ, T, 1)
end
function etransition(f_logÏ€q, f_gradlogÏ€, M, Mâ»Â¹, Ïµ, T)
    qâ‚€ = targetrand(Mâ»Â¹)
    etransition(f_logÏ€q, f_gradlogÏ€, qâ‚€, M, Mâ»Â¹, Ïµ, T)
end


# Version which explicitly constructs the trajectories, then samples them
function etransition2!(f_logÏ€q, f_gradlogÏ€, tâ‚€, tâ€², qâ‚€, pâ‚€, M, Mâ»Â¹, wâ‚€, Ïµ, T, L)
    if Ïµ * L â‰¥ T
        return qâ‚€, pâ‚€
    else
        Ïµ = rand(Bool) ? -Ïµ : Ïµ
        resize!(tâ€², L)
        wâ€² = 0.0
        pâ€² = pâ‚€ + 0.5Ïµ * f_gradlogÏ€(qâ‚€)
        qâ€² = qâ‚€ + Ïµ * Mâ»Â¹ * pâ€²
        for l = 2:L
            pâ€² = pâ€² + Ïµ * f_gradlogÏ€(qâ€²)
            tâ€²[l - 1] = (qâ€², pâ€²)
            wâ€² += exp(-H(f_logÏ€q, qâ€², pâ€², Mâ»Â¹))
            qâ€² = qâ€² + Ïµ * Mâ»Â¹ * pâ€²
        end
        pâ€² = pâ€² + 0.5Ïµ * f_gradlogÏ€(qâ€²)
        tâ€²[L] = (qâ€², pâ€²)
        wâ€² += exp(-H(f_logÏ€q, qâ€², pâ€², Mâ»Â¹))
        ğ“…â‚€ = wâ‚€ / (wâ‚€ + wâ€²)
        if rand() â‰¤ ğ“…â‚€
            q, p = rand(tâ‚€)
            signbit(Ïµ) ? prepend!(tâ‚€, tâ€²) : append!(tâ‚€, tâ€²)
            return etransition2!(f_logÏ€q, f_gradlogÏ€, tâ‚€, tâ€², q, p, M, Mâ»Â¹, wâ‚€ + wâ€², abs(Ïµ), T, 2L)
        else
            q, p = rand(tâ€²)
            signbit(Ïµ) ? prepend!(tâ‚€, tâ€²) : append!(tâ‚€, tâ€²)
            return etransition2!(f_logÏ€q, f_gradlogÏ€, tâ‚€, tâ€², q, p, M, Mâ»Â¹, wâ‚€ + wâ€², abs(Ïµ), T, 2L)
        end
    end
end
function etransition2(f_logÏ€q, f_gradlogÏ€, qâ‚€, M, Mâ»Â¹, Ïµ, T)
    pâ‚€ = momentumrand_M(M)
    wâ‚€ = exp(-H(f_logÏ€q, qâ‚€, pâ‚€, Mâ»Â¹))
    tâ‚€ = Vector{Tuple{Vector{Float64}, Vector{Float64}}}(undef, 0)
    tâ€² = Vector{Tuple{Vector{Float64}, Vector{Float64}}}(undef, 0)
    push!(tâ‚€, (qâ‚€, pâ‚€))
    etransition2!(f_logÏ€q, f_gradlogÏ€, tâ‚€, tâ€², qâ‚€, pâ‚€, M, Mâ»Â¹, wâ‚€, Ïµ, T, 1)
end
function etransition2(f_logÏ€q, f_gradlogÏ€, M, Mâ»Â¹, Ïµ, T)
    qâ‚€ = targetrand(Mâ»Â¹)
    etransition2(f_logÏ€q, f_gradlogÏ€, qâ‚€, M, Mâ»Â¹, Ïµ, T)
end


function ehmc(f_logÏ€q, f_gradlogÏ€, qâ‚€, M, Mâ»Â¹, Ïµ, T, N)
    Î¸ = Matrix{Float64}(undef, size(M, 1), N)
    E = Vector{Float64}(undef, N)
    n, t = 0, 0
    while n < N
        qâ€², pâ€² = etransition(f_logÏ€, f_gradlogÏ€, qâ‚€, M, Mâ»Â¹, Ïµ, T)
        if qâ‚€ != qâ€²
            qâ‚€ = qâ€²
            n += 1
            Î¸[:, n] = qâ€²
            E[n] = H(f_logÏ€q, qâ€², pâ€², Mâ»Â¹)
        end
        t += 1
    end
    Î¸, E, n, t
end
ehmc(f_logÏ€q, f_gradlogÏ€, M, Mâ»Â¹, Ïµ, T, N) =
    ehmc(f_logÏ€q, f_gradlogÏ€, targetrand(Mâ»Â¹), M, Mâ»Â¹, Ïµ, T, N)

Ïµ = 0.125
L = 8
N = 1000
N_chains = 4

@timev etransition(f_logÏ€, f_gradlogÏ€, M, Mâ»Â¹, Ïµ, Ïµ * L)
@timev etransition2(f_logÏ€, f_gradlogÏ€, M, Mâ»Â¹, Ïµ, Ïµ * L)

@timev Î¸, E, n, t = ehmc(f_logÏ€, f_gradlogÏ€, M, Mâ»Â¹, Ïµ, Ïµ * L, N)
Ï‘ = originalparam(Î¸);
mean(Ï‘, dims=2)

ebfmi(E)

Epq, Eq = microcanonicalenergy(f_logÏ€, Î¸, E);
pâ‚• = histogram([Epq Eq] .- [mean(Epq) mean(Eq)], labels=["E_pq â‰¡ Ï€(E|q)" "E_q â‰¡ Ï€(E)"], alpha=0.5,
               annotations=((0.1, 1.0), text("E-BFMI = $(round(ebfmi(E), digits=3))", 8)),
               xlabel="E - <E>");
savefig(pâ‚•, joinpath(pwd(), "energy_ehmc.pdf"))

@timev chains = [ehmc(f_logÏ€, f_gradlogÏ€, M, Mâ»Â¹, Ïµ, Ïµ * L, N) for _ = 1:N_chains];
acprob = getindex.(chains, 3) ./ getindex.(chains, 4)
Î¸ = cat(first.(chains)..., dims=3);
mean(Î¸, dims=(2,3))
E = hcat(getindex.(chains, 2)...);

Ï‘â‚€ = originalparam(Î¸);
Ï‘ = originalparam(Î¸[:, (N >> 1 + 1):N, :]);
mean(Ï‘, dims=(2, 3))
# Convergence check
nâ‚‚ = size(Ï‘, 2)
n = nâ‚‚ >> 1
Ï‘â‚‚ = [view(Ï‘, :, 1:n, :);;; view(Ï‘, :, (n + 1):nâ‚‚, :)];
neff(Ï‘â‚‚)
Rhat(Ï‘â‚‚)

############################################################################################
#### 2022-02-17: dynamic HMC
function dotdiff(pâ€²::Vector{T}, qâ€²::Vector{T}, qâ‚€::Vector{T}) where {T<:Real}
    s = zero(T)
    @inbounds @simd for i âˆˆ eachindex(pâ€², qâ€², qâ‚€)
        Î”q = qâ€²[i] - qâ‚€[i]
        s += pâ€²[i] * Î”q
    end
    s
end
function nutscriterion(qâ€², pâ€², qâ‚€, pâ‚€)
    # Î”q = qâ€² - qâ‚€
    # pâ€² â‹… Î”q < 0.0 && pâ‚€ â‹… Î”q > 0.0
    # pâ€² â‹… (qâ€² - qâ‚€) < 0.0 && pâ‚€ â‹… (qâ‚€ - qâ€²) < 0.0
    dotdiff(pâ€², qâ€², qâ‚€) < 0.0 && dotdiff(pâ‚€, qâ‚€, qâ€²) < 0.0
end

function momentumstep!(pâ€²::Vector{T}, Ïµ::T, gradlogÏ€q::Vector{T}) where {T<:Real}
    @inbounds @simd for i âˆˆ eachindex(pâ€², gradlogÏ€q)
        pâ€²[i] += Ïµ * gradlogÏ€q[i]
    end
    pâ€²
end

function targetstepdiag!(qâ€²::Vector{T}, Ïµ::T, Mâ»Â¹::Matrix{T}, pâ€²::Vector{T}) where {T<:Real}
    @inbounds @simd for i âˆˆ eachindex(qâ€², pâ€²)
        qâ€²[i] += Ïµ * Mâ»Â¹[i, i] * pâ€²[i]
    end
    qâ€²
end

function dtransition(f_logÏ€q, f_gradlogÏ€, qâ‚€, pâ‚€, M, Mâ»Â¹, wâ‚€, Ïµ, L, Lâ‚˜)
    L â‰¥ Lâ‚˜ && return qâ‚€, pâ‚€, L
    Ïµ = rand(Bool) ? -Ïµ : Ïµ
    wâ€² = 0.0
    pâ€² = pâ‚€ + 0.5Ïµ * f_gradlogÏ€(qâ‚€)
    qâ€² = qâ‚€ + Ïµ * Mâ»Â¹ * pâ€²
    #### Version for clarity
    # for l = 2:L
    #     pâ€² = pâ€² + Ïµ * f_gradlogÏ€(qâ€²)
    #     # nutscriterion(qâ€², pâ€², qâ‚€, pâ‚€) && return dtransition(f_logÏ€q, f_gradlogÏ€, qâ‚€, pâ‚€, M, Mâ»Â¹, wâ‚€, Ïµ, 2L)
    #     wâ€² += exp(-H(f_logÏ€q, qâ€², pâ€², Mâ»Â¹))
    #     # nutscriterion(qâ€², pâ€², qâ‚€, pâ‚€) && return rand() â‰¤ wâ‚€ / (wâ‚€ + wâ€²) ? (qâ‚€, pâ‚€) : (qâ€², pâ€²)
    #     qâ€² = qâ€² + Ïµ * Mâ»Â¹ * pâ€²
    # end
    #### mutating version
    for l = 2:L
        momentumstep!(pâ€², Ïµ, f_gradlogÏ€(qâ€²))
        wâ€² += exp(-H(f_logÏ€q, qâ€², pâ€², Mâ»Â¹))
        targetstepdiag!(qâ€², Ïµ, Mâ»Â¹, pâ€²)
    end
    ####
    # pâ€² = pâ€² + 0.5Ïµ * f_gradlogÏ€(qâ€²)
    momentumstep!(pâ€², 0.5Ïµ, f_gradlogÏ€(qâ€²))
    wâ€² += exp(-H(f_logÏ€q, qâ€², pâ€², Mâ»Â¹))
    # ğ“…â‚€ = wâ‚€ / (wâ‚€ + wâ€²)
    # turned = nutscriterion(qâ€², pâ€², qâ‚€, pâ‚€)
    # if rand() â‰¤ ğ“…â‚€
    #     # return dtransition(f_logÏ€q, f_gradlogÏ€, qâ‚€, rand(Bool) ? -pâ‚€ : pâ‚€,
    #     #                    M, Mâ»Â¹, wâ‚€ + wâ€², Ïµ, T, 2L)
    #     return turned ? (qâ‚€, pâ‚€) : dtransition(f_logÏ€q, f_gradlogÏ€, qâ‚€, pâ‚€, M, Mâ»Â¹, wâ‚€ + wâ€², Ïµ, 2L)
    # else
    #     # return dtransition(f_logÏ€q, f_gradlogÏ€, qâ€², rand(Bool) ? -pâ€² : pâ€²,
    #     #                    M, Mâ»Â¹, wâ‚€ + wâ€², Ïµ, T, 2L)
    #     return turned ? (qâ€², pâ€²) : dtransition(f_logÏ€q, f_gradlogÏ€, qâ€², pâ€², M, Mâ»Â¹, wâ‚€ + wâ€², Ïµ, 2L)
    # end
    if nutscriterion(qâ€², pâ€², qâ‚€, pâ‚€)
        return qâ€², pâ€², L
    else
        ğ“…â‚€ = wâ‚€ / (wâ‚€ + wâ€²)
        return rand() â‰¤ ğ“…â‚€ ? dtransition(f_logÏ€q, f_gradlogÏ€, qâ‚€, pâ‚€, M, Mâ»Â¹, wâ‚€ + wâ€², Ïµ, 2L, Lâ‚˜) :
            dtransition(f_logÏ€q, f_gradlogÏ€, qâ€², pâ€², M, Mâ»Â¹, wâ‚€ + wâ€², Ïµ, 2L, Lâ‚˜)
    end
end
function dtransition(f_logÏ€q, f_gradlogÏ€, qâ‚€, M, Mâ»Â¹, Ïµ, Lâ‚˜)
    pâ‚€ = momentumrand_M(M)
    wâ‚€ = exp(-H(f_logÏ€q, qâ‚€, pâ‚€, Mâ»Â¹))
    dtransition(f_logÏ€q, f_gradlogÏ€, qâ‚€, pâ‚€, M, Mâ»Â¹, wâ‚€, Ïµ, 1, Lâ‚˜)
end
function dtransition(f_logÏ€q, f_gradlogÏ€, M, Mâ»Â¹, Ïµ, Lâ‚˜)
    qâ‚€ = targetrand(Mâ»Â¹)
    pâ‚€ = momentumrand_M(M)
    wâ‚€ = exp(-H(f_logÏ€q, qâ‚€, pâ‚€, Mâ»Â¹))
    dtransition(f_logÏ€q, f_gradlogÏ€, qâ‚€, pâ‚€, M, Mâ»Â¹, wâ‚€, Ïµ, 1, Lâ‚˜)
end

function dhmc(f_logÏ€q, f_gradlogÏ€, qâ‚€, M, Mâ»Â¹, Ïµ, Lâ‚˜, N)
    Î¸ = Matrix{Float64}(undef, size(M, 1), N)
    E = Vector{Float64}(undef, N)
    depth = Vector{Int}(undef, N)
    n, t = 0, 0
    while n < N
        # qâ€², pâ€², L = dtransition(f_logÏ€, f_gradlogÏ€, qâ‚€, M, Mâ»Â¹, Ïµ, Lâ‚˜)
        # if qâ‚€ != qâ€²
        #     qâ‚€ = qâ€²
        #     n += 1
        #     Î¸[:, n] = qâ€²
        #     E[n] = H(f_logÏ€q, qâ€², pâ€², Mâ»Â¹)
        #     depth[n] = L
        # end
        # t += 1
        pâ‚€ = momentumrand_M(M)
        wâ‚€ = exp(-H(f_logÏ€q, qâ‚€, pâ‚€, Mâ»Â¹))
        qâ€², pâ€², L = dtransition(f_logÏ€, f_gradlogÏ€, qâ‚€, pâ‚€, M, Mâ»Â¹, wâ‚€, Ïµ, 1, Lâ‚˜)
        Eâ‚€ = H(f_logÏ€q, qâ‚€, pâ‚€, Mâ»Â¹)
        Eâ€² = H(f_logÏ€q, qâ€², pâ€², Mâ»Â¹)
        r = exp(-Eâ‚€ + Eâ€²)
        if rand() < r
            n += 1
            Î¸[:, n] = qâ€²
            qâ‚€ = qâ€²
            E[n] = Eâ€²
            depth[n] = L
        end
        t += 1
    end
    Î¸, E, n, t, depth
end
dhmc(f_logÏ€q, f_gradlogÏ€, M, Mâ»Â¹, Ïµ, Lâ‚˜, N) =
    dhmc(f_logÏ€q, f_gradlogÏ€, targetrand(Mâ»Â¹), M, Mâ»Â¹, Ïµ, Lâ‚˜, N)

Ïµ = 0.01
Lâ‚˜ = 2^10
N = 1000
N_chains = 4

@timev dtransition(f_logÏ€, f_gradlogÏ€, M, Mâ»Â¹, Ïµ, Lâ‚˜)

@timev Î¸, E, n, t, depth = dhmc(f_logÏ€, f_gradlogÏ€, M, Mâ»Â¹, Ïµ, Lâ‚˜, N);
@timev Î¸, E, n, t, depth = dhmc(f_logÏ–, f_gradlogÏ–, M, Mâ»Â¹, Ïµ, Lâ‚˜, N);
Ï‘ = originalparam(Î¸);
mean(Ï‘, dims=2)
mean(log2, depth)
maximum(log2, depth)

ebfmi(E)

Epq, Eq = microcanonicalenergy(f_logÏ€, Î¸, E);
pâ‚• = histogram([Epq Eq] .- [mean(Epq) mean(Eq)], labels=["E_pq â‰¡ Ï€(E|q)" "E_q â‰¡ Ï€(E)"], alpha=0.5,
               annotations=((0.1, 1.0), text("E-BFMI = $(round(ebfmi(E), digits=3))", 8)),
               xlabel="E - <E>");
savefig(pâ‚•, joinpath(pwd(), "energy_dhmc.pdf"))

@timev chains = [dhmc(f_logÏ€, f_gradlogÏ€, M, Mâ»Â¹, Ïµ, Lâ‚˜, N) for _ = 1:N_chains];
acprob = getindex.(chains, 3) ./ getindex.(chains, 4)
Î¸ = cat(first.(chains)..., dims=3);
mean(Î¸, dims=(2,3))
E = hcat(getindex.(chains, 2)...);

Ï‘â‚€ = originalparam(Î¸);
Ï‘ = originalparam(Î¸[:, (N >> 1 + 1):N, :]);
mean(Ï‘, dims=(2, 3))
# Convergence check
nâ‚‚ = size(Ï‘, 2)
n = nâ‚‚ >> 1
Ï‘â‚‚ = [view(Ï‘, :, 1:n, :);;; view(Ï‘, :, (n + 1):nâ‚‚, :)];
neff(Ï‘â‚‚)
Rhat(Ï‘â‚‚)

ps = map(1:N_chains) do k
    Epq, Eq = microcanonicalenergy(f_logÏ€, Î¸[:, :, k], E[:, k])
    pâ‚• = histogram([Epq Eq] .- [mean(Epq) mean(Eq)], labels=["E_pq â‰¡ Ï€(E|q)" "E_q â‰¡ Ï€(E)"],
                   alpha=0.5,
                   annotations=((0.1, 1.0), text("E-BFMI = $(round(ebfmi(E[:, k]), digits=3))", 8)),
                   xlabel="E - <E>")
    pâ‚•
end;
pâ‚• = plot(ps...);
savefig(pâ‚•, joinpath(pwd(), "dhmc_energy_$(N_chains)chain.pdf"))

# Neat trace plots
ps = map(1:N_chains) do k
    plot(permutedims(Ï‘â‚€[:, :, k]), labels=["Î¸$(tosubscript(i))" for _ = 1:1, i = 1:10])
end;
p = plot(ps..., link=:all);
savefig(p, joinpath(pwd(), "dhmc_$(N_chains)chains_trace.pdf"))

# autocorrelation plots
Ï‘â±¼ = Ï‘[1, :];
acf = map(0:500) do k
    acor(Ï‘â±¼, k)
end;
p = bar(acf, label="empirical")
savefig(p, joinpath(pwd(), "dhmc_autocorrelation.pdf"))
#
ps = map(1:size(Ï‘, 1)) do j
    Ï‘â±¼ = Ï‘[j, :]
    acf = map(0:500) do k
        acor(Ï‘â±¼, k)
    end;
    p = bar(acf, label="Î¸$(j)")
    p
end;
p = plot(ps...);
savefig(p, joinpath(pwd(), "dhmc_autocorrelation.pdf"))
