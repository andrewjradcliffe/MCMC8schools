#
# Date created: 2022-02-08
# Author: aradclif
#
#
############################################################################################
# Centered parameterization:
# yâ±¼ ~ N(Î±â±¼, Ïƒâ±¼Â²)
# Î±â±¼ ~ N(Î¼, Ï„Â²)
# Î¼ ~ U(-âˆ, âˆ)
# Ï„ ~ U(0, âˆ)
# Ï„ transformed to logÏ„ to place on unconstrained space

function gradlogÏ€(Î±, Î¼, logÏ„, y, ÏƒÂ²)
    Ï„Â² = abs2(exp(logÏ„))
    # âˆ‡l = @. - (Î± - y) / ÏƒÂ² - (Î± - Î¼) / Ï„Â²
    âˆ‡l = .- (Î± .- y) ./ ÏƒÂ² .- (Î± .- Î¼) ./ Ï„Â²
    dÏ€dÎ¼ = 0.
    for j âˆˆ axes(Î±, 1)
        dÏ€dÎ¼ -= (Î¼ - Î±[j]) / Ï„Â² # -
    end
    dÏ€dlogÏ„ = - length(Î±) + 1.
    for j âˆˆ axes(Î±, 1)
        dÏ€dlogÏ„ += abs2(Î±[j] - Î¼) / Ï„Â²
    end
    [âˆ‡l; dÏ€dÎ¼; dÏ€dlogÏ„]
end
gradlogÏ€(q, y, ÏƒÂ²) = gradlogÏ€(q[1:8], q[9], q[10], y, ÏƒÂ²)

function logÏ€q(Î±, Î¼, logÏ„, y, ÏƒÂ²)
    Ï„Â² = abs2(exp(logÏ„))
    # s = 0
    # s += logÏ„
    s = logÏ„
    for j âˆˆ axes(Î±, 1)
        s += -log(âˆš(2Ï€ * ÏƒÂ²[j])) - 0.5abs2(y[j] - Î±[j]) / ÏƒÂ²[j] - 0.5abs2(Î±[j] - Î¼) / Ï„Â²
        # s += - 0.5log(2Ï€ * ÏƒÂ²[j]) - 0.5abs2(y[j] - Î±[j]) / ÏƒÂ²[j] - 0.5abs2(Î±[j] - Î¼) / Ï„Â²
    end
    s -= length(Î±) * log(âˆš(2Ï€ * Ï„Â²))
    # s -= length(Î±) * 0.5log(2Ï€ * Ï„Â²)
    s
    # # unnormalized density
    # Ï„Â² = abs2(exp(logÏ„))
    # s = logÏ„
    # for j âˆˆ axes(Î±, 1)
    #     s += -0.5abs2(y[j] - Î±[j]) / ÏƒÂ²[j] - 0.5abs2(Î±[j] - Î¼) / Ï„Â²
    # end
    # s += -0.5log(2Ï€ * Ï„Â²)
    # s
end
logÏ€q(q, y, ÏƒÂ²) = logÏ€q(q[1:8], q[9], q[10], y, ÏƒÂ²)

function inversediagdet(Mâ»Â¹::Matrix{T}) where {T<:Real}
    s = one(T)
    for j âˆˆ axes(Mâ»Â¹, 1)
        s *= one(T) / Mâ»Â¹[j, j]
    end
    s
end

function weightedss(p, Mâ»Â¹::Matrix{T}) where {T<:Real}
    s = zero(T)
    for j âˆˆ axes(Mâ»Â¹, 1)
        s += Mâ»Â¹[j, j] * abs2(p[j])
        # or:
        # pâ±¼ = p[j]
        # s += pâ±¼ * pâ±¼ * Mâ»Â¹[j, j]
    end
    s
end

function logÏ€p(p, Mâ»Â¹)
    # s = length(p) * log(2Ï€)
    # s += log(inversediagdet(Mâ»Â¹))
    # s += weightedss(p, Mâ»Â¹)
    # s *= -0.5
    # s
    # # Or, simpler:
    -0.5 * (length(p) * log(2Ï€) + log(inversediagdet(Mâ»Â¹)) + weightedss(p, Mâ»Â¹))
    # # unnormalized density
    # -0.5weightedss(p, Mâ»Â¹) -0.5log(inversediagdet(Mâ»Â¹))
    # -0.5weightedss(p, Mâ»Â¹)
end

function symplecticintegrate(qâ‚€, pâ‚€, Mâ»Â¹, Ïµ, L, y, ÏƒÂ²)
    # # Option 1
    # qâ€², pâ€² = qâ‚€, pâ‚€
    # pâ€² = pâ€² + 0.5Ïµ * gradlogÏ€(qâ‚€, y, ÏƒÂ²) # -
    # for l = 2:L
    #     qâ€² = qâ€² + Ïµ * Mâ»Â¹ * pâ€²
    #     pâ€² = pâ€² + Ïµ * gradlogÏ€(qâ€², y, ÏƒÂ²) # -
    # end
    # qâ€² = qâ€² + Ïµ * Mâ»Â¹ * pâ€²
    # pâ€² = pâ€² + 0.5Ïµ * gradlogÏ€(qâ€², y, ÏƒÂ²) # -
    # # Option 2
    pâ€² = pâ‚€ + 0.5Ïµ * gradlogÏ€(qâ‚€, y, ÏƒÂ²) # -
    qâ€² = qâ‚€ + Ïµ * Mâ»Â¹ * pâ€²
    for l = 2:L
        pâ€² = pâ€² + Ïµ * gradlogÏ€(qâ€², y, ÏƒÂ²) # -
        qâ€² = qâ€² + Ïµ * Mâ»Â¹ * pâ€²
    end
    pâ€² = pâ€² + 0.5Ïµ * gradlogÏ€(qâ€², y, ÏƒÂ²) # -
    qâ€², -pâ€²
end

function logratio(qâ€², pâ€², qâ‚€, pâ‚€, Mâ»Â¹, y, ÏƒÂ²)
    logÏ€q(qâ€², y, ÏƒÂ²) + logÏ€p(pâ€², Mâ»Â¹) - logÏ€q(qâ‚€, y, ÏƒÂ²) - logÏ€p(pâ‚€, Mâ»Â¹)
end

function metropolis(qâ€², pâ€², qâ‚€, pâ‚€, Mâ»Â¹, y, ÏƒÂ²)
    r = exp(logratio(qâ€², pâ€², qâ‚€, pâ‚€, Mâ»Â¹, y, ÏƒÂ²))
    rand() < r ? (qâ€², pâ€²) : (qâ‚€, pâ‚€)
end

# Using the mass matrix directly
function momentumrand_M(M)
    p = randn(size(M, 1))
    @inbounds for j âˆˆ axes(M, 1)
        p[j] *= âˆš(M[j, j])
    end
    p
end

# Using the lower triangular of the cholesky, AAáµ€ = M
function momentumrand_A(A)
    p = randn(size(A, 1))
    @inbounds for j âˆˆ axes(A, 1)
        p[j] *= A[j, j]
    end
    p
end

# Using the inverse mass matrix directly
function momentumrand_Mâ»Â¹(Mâ»Â¹)
    p = randn(size(Mâ»Â¹, 1))
    @inbounds for j âˆˆ axes(Mâ»Â¹, 1)
        p[j] *= inv(âˆš(Mâ»Â¹[j, j]))
    end
    p
end

function hmc(Ïµ, L, M, Mâ»Â¹, Î±â‚€, Î¼â‚€, logÏ„â‚€, y, ÏƒÂ²)
    qâ‚€ = [Î±â‚€; Î¼â‚€; logÏ„â‚€]
    pâ‚€ = momentumrand_M(M)
    qâ€², pâ€² = symplecticintegrate(qâ‚€, pâ‚€, Mâ»Â¹, Ïµ, L, y, ÏƒÂ²)
    metropolis(qâ€², pâ€², qâ‚€, pâ‚€, Mâ»Â¹, y, ÏƒÂ²)
end

function hmc(N, Ïµ, L, M, Mâ»Â¹, Î±â‚€, Î¼â‚€, logÏ„â‚€, y, ÏƒÂ²)
    qâ‚€ = [Î±â‚€; Î¼â‚€; logÏ„â‚€]
    # Î¸ = Vector{Vector{Float64}}()
    Î¸ = Matrix{Float64}(undef, 10, N)
    n, t = 0, 0
    while n < N
        pâ‚€ = momentumrand_M(M)
        qâ€², pâ€² = symplecticintegrate(qâ‚€, pâ‚€, Mâ»Â¹, Ïµ, L, y, ÏƒÂ²)
        r = exp(logÏ€q(qâ€², y, ÏƒÂ²) + logÏ€p(pâ€², Mâ»Â¹) - logÏ€q(qâ‚€, y, ÏƒÂ²) - logÏ€p(pâ‚€, Mâ»Â¹))
        if rand() < r
            # push!(Î¸, qâ€²)
            qâ‚€ = qâ€²
            n += 1
            t += 1
            Î¸[:, n] = qâ€²
        else
            t += 1
        end
    end
    Î¸, n, t
end

function hmc(N, Ïµ, L, M, Mâ»Â¹, y, ÏƒÂ²)
    J = length(y)
    qâ‚€, t = initial(Ïµ, L, M, Mâ»Â¹, y, ÏƒÂ²)
    Î±â‚€, Î¼â‚€, logÏ„â‚€ = qâ‚€[1:J], qâ‚€[J + 1], qâ‚€[J + 2]
    hmc(N, Ïµ, L, M, Mâ»Â¹, Î±â‚€, Î¼â‚€, logÏ„â‚€, y, ÏƒÂ²)
end

function initial(Ïµ, L, M, Mâ»Â¹, y, ÏƒÂ²)
    J = length(y)
    ğ“ˆ = sqrt.(diag(Mâ»Â¹))
    t = 0
    while true
        Î±â‚€ = randn(J) .* ğ“ˆ[1:J]
        Î¼â‚€ = randn() * ğ“ˆ[J + 1]
        logÏ„â‚€ = randn() * ğ“ˆ[J + 2]
        qâ‚€ = [Î±â‚€; Î¼â‚€; logÏ„â‚€]
        qâ€², pâ€² = hmc(Ïµ, L, M, Mâ»Â¹, Î±â‚€, Î¼â‚€, logÏ„â‚€, y, ÏƒÂ²)
        t += 1
        qâ€² != qâ‚€ && break
    end
    return qâ€², t
end

################ Data
y = Float64[28, 8, -3, 7, -1, 1, 18, 12];
Ïƒ = Float64[15, 10, 16, 11, 9, 11, 10, 18];
ÏƒÂ² = abs2.(Ïƒ);
################ Putting it all together
# Initialize
J = length(y)
# # Stan's initialization -- far too restrictive for this model
# Î±â‚€ = rand(J) .* 4 .- 2
# Î¼â‚€ = rand() * 4 - 2
# logÏ„â‚€ = log(rand() * 15)
# # Default initialization
scale = 15;
# Î±â‚€ = randn(J) .* scale;
# Î¼â‚€ = randn() * scale;
# logÏ„â‚€ = randn();
# Specify Ïµ, L, N
Ïµ = 0.1
L = 10
N = 1000
#
# M = diagm(scale*(ones(10))); M[10, 10] = 1;
# Mâ»Â¹ = diagm(1 ./ diag(M));
# Approximate M: inverse of covariance matrix of posterior
M = diagm(fill(1 / scale^2, 10)); M[10, 10] = 1;
Mâ»Â¹ = diagm(1 ./ diag(M));

# Initialization tests -- guarantee a decent starting point
qâ‚€, t = initial(Ïµ, L, M, Mâ»Â¹, y, ÏƒÂ²)
Î±â‚€, Î¼â‚€, logÏ„â‚€ = qâ‚€[1:J], qâ‚€[J + 1], qâ‚€[J + 2]
####

# Single step, fixed initialization
@timev q, p = hmc(Ïµ, L, M, Mâ»Â¹, Î±â‚€, Î¼â‚€, logÏ„â‚€, y, ÏƒÂ²)
# Fixed initialization
@timev Î¸, n, t = hmc(N, Ïµ, L, M, Mâ»Â¹, Î±â‚€, Î¼â‚€, logÏ„â‚€, y, ÏƒÂ²);

# Auto-initialization
@timev Î¸, n, t = hmc(N, Ïµ, L, M, Mâ»Â¹, y, ÏƒÂ²);
acprob = n / t
mean(Î¸, dims=2)
mean(Î¸[:, (N >> 1):N], dims=2)

# Multiple chains
@timev chains = [hmc(N, Ïµ, L, M, Mâ»Â¹, y, ÏƒÂ²) for _ = 1:4];
acprob = getindex.(chains, 2) ./ getindex.(chains, 3)
Î¸ = cat(first.(chains)..., dims=3);
mean(Î¸, dims=(2,3))

# Straightforward tests
gradlogÏ€(Î±â‚€, Î¼â‚€, logÏ„â‚€, y, ÏƒÂ²)
@benchmark momentumrand_M(M)
A, Aáµ€ = cholesky(M)
momentumrand_Mâ»Â¹(Mâ»Â¹)
@benchmark momentumrand_A(A)
A * randn(10)
inversediagdet(Mâ»Â¹)
@benchmark weightedss(p, Mâ»Â¹)
@benchmark weightedss2(p, Mâ»Â¹)

# Trajectory test
qâ‚€, pâ‚€ = [Î±â‚€; Î¼â‚€; logÏ„â‚€], momentumrand_M(M);
qâ€², pâ€² = symplecticintegrate(qâ‚€, pâ‚€, Mâ»Â¹, Ïµ, L, y, ÏƒÂ²)
logÏ€q(qâ€², y, ÏƒÂ²)
logÏ€p(pâ€², Mâ»Â¹)
logratio(qâ€², pâ€², qâ‚€, pâ‚€, Mâ»Â¹, y, ÏƒÂ²)


# normpdf(x) = inv(âˆš(2Ï€)) * exp(-0.5 * x * x)
# lognormpdf(x) = -0.5log(2Ï€) - 0.5 * x * x
# dlognormpdf(x) = -x

############################################################################################
#### 2022-02-14: non-centered parameterization
# yâ±¼ ~ N(Î¼ + Ï„ * Î·â±¼, Ïƒâ±¼Â²)
# Î·â±¼ ~ N(0, 1)
# Î¼ ~ U(-âˆ, âˆ)
# Ï„ ~ U(0, âˆ)
# Ï„ transformed to logÏ„ to place on unconstrained space

# Clearly not the most efficient approach -- the proper way is to compute a vector, the
# elements of which are (y[j] - Î¼ - Ï„ * Î·[j]). Then, compute dÏ€dÎ¼ and dÏ€dlogÏ„, then
# mutate in place to complete the computation of âˆ‡l.
function gradlogÏ€_nc(Î·, Î¼, logÏ„, y, ÏƒÂ²)
    Ï„ = exp(logÏ„)
    âˆ‡l = Vector{Float64}(undef, length(Î·))
    for j âˆˆ eachindex(Î·, y, ÏƒÂ²)
        âˆ‡l[j] = (y[j] - Î¼ - Ï„ * Î·[j]) * Ï„ / ÏƒÂ²[j] - Î·[j]
    end
    dÏ€dÎ¼ = 0.0
    for j âˆˆ eachindex(Î·, y, ÏƒÂ²)
        dÏ€dÎ¼ += (y[j] - Î¼ - Ï„ * Î·[j]) / ÏƒÂ²[j]
    end
    dÏ€dlogÏ„ = 1.0
    for j âˆˆ eachindex(Î·, y, ÏƒÂ²)
        dÏ€dlogÏ„ += Ï„ * (y[j] - Î¼ - Ï„ * Î·[j]) * Î·[j] / ÏƒÂ²[j]
    end
    [âˆ‡l; dÏ€dÎ¼; dÏ€dlogÏ„]
end
gradlogÏ€_nc(q, y, ÏƒÂ²) = gradlogÏ€_nc(q[1:8], q[9], q[10], y, ÏƒÂ²)

function logÏ€q_nc(Î·, Î¼, logÏ„, y, ÏƒÂ²)
    Ï„ = exp(logÏ„)
    s = logÏ„
    for j âˆˆ eachindex(Î·, y, ÏƒÂ²)
        s += -0.5(log(2Ï€ * ÏƒÂ²[j]) + (1.0 / ÏƒÂ²[j]) * abs2(y[j] - Î¼ - Ï„ * Î·[j]) + log(2Ï€) + abs2(Î·[j]))
    end
    s
end
logÏ€q_nc(q, y, ÏƒÂ²) = logÏ€q_nc(q[1:8], q[9], q[10], y, ÏƒÂ²)

function symplecticintegrate(f_gradlogÏ€, qâ‚€, pâ‚€, Mâ»Â¹, Ïµ, L, y, ÏƒÂ²)
    pâ€² = pâ‚€ + 0.5Ïµ * f_gradlogÏ€(qâ‚€, y, ÏƒÂ²)
    qâ€² = qâ‚€ + Ïµ * Mâ»Â¹ * pâ€²
    for l = 2:L
        pâ€² = pâ€² + Ïµ * f_gradlogÏ€(qâ€², y, ÏƒÂ²)
        qâ€² = qâ€² + Ïµ * Mâ»Â¹ * pâ€²
    end
    pâ€² = pâ€² + 0.5Ïµ * f_gradlogÏ€(qâ€², y, ÏƒÂ²)
    qâ€², -pâ€²
end

function targetrand(Mâ»Â¹::Matrix{T}) where {T<:Real}
    K = size(Mâ»Â¹, 1)
    qâ‚€ = randn(K)
    for k âˆˆ eachindex(qâ‚€)
        qâ‚€[k] *= âˆš(Mâ»Â¹[k, k])
    end
    qâ‚€
end

function hmc_nc_transition(f_gradlogÏ€, f_logÏ€q, Ïµ, L, M, Mâ»Â¹, qâ‚€, y, ÏƒÂ²)
    pâ‚€ = momentumrand_M(M)
    qâ€², pâ€² = symplecticintegrate(f_gradlogÏ€, qâ‚€, pâ‚€, Mâ»Â¹, Ïµ, L, y, ÏƒÂ²)
    r = exp(f_logÏ€q(qâ€², y, ÏƒÂ²) + logÏ€p(pâ€², Mâ»Â¹) - f_logÏ€q(qâ‚€, y, ÏƒÂ²) - logÏ€p(pâ‚€, Mâ»Â¹))
    u = rand()
    u < r ? (qâ€², pâ€²) : (qâ‚€, pâ‚€)
end
function hmc_nc_transition(f_gradlogÏ€, f_logÏ€q, Ïµ, L, M, Mâ»Â¹, y, ÏƒÂ²)
    qâ‚€ = targetrand(Mâ»Â¹)
    hmc_nc_transition(f_gradlogÏ€, f_logÏ€q, Ïµ, L, M, Mâ»Â¹, qâ‚€, y, ÏƒÂ²)
end

function hmc_nc(f_gradlogÏ€, f_logÏ€q, N, Ïµ, L, M, Mâ»Â¹, qâ‚€, y, ÏƒÂ²)
    Î¸ = Matrix{Float64}(undef, length(qâ‚€), N)
    n, t = 0, 0
    while n < N
        pâ‚€ = momentumrand_M(M)
        qâ€², pâ€² = symplecticintegrate(f_gradlogÏ€, qâ‚€, pâ‚€, Mâ»Â¹, Ïµ, L, y, ÏƒÂ²)
        r = exp(f_logÏ€q(qâ€², y, ÏƒÂ²) + logÏ€p(pâ€², Mâ»Â¹) - f_logÏ€q(qâ‚€, y, ÏƒÂ²) - logÏ€p(pâ‚€, Mâ»Â¹))
        if rand() < r
            qâ‚€ = qâ€²
            n += 1
            t += 1
            Î¸[:, n] = qâ€²
        else
            t += 1
        end
    end
    Î¸, n, t
end

function hmc_nc(f_gradlogÏ€, f_logÏ€q, N, Ïµ, L, M, Mâ»Â¹, y, ÏƒÂ²)
    qâ‚€ = targetrand(Mâ»Â¹)
    hmc_nc(f_gradlogÏ€, f_logÏ€q, N, Ïµ, L, M, Mâ»Â¹, qâ‚€, y, ÏƒÂ²)
end

# #### Unlike the centered parameterization, the non-centered parameterization
# will not get stuck at low values of Ï„, hence, no need to guarantee a "decent" starting point.
# function initial_nc(f_gradlogÏ€, f_logÏ€q, Ïµ, L, M, Mâ»Â¹, y, ÏƒÂ²)
#     K = size(Mâ»Â¹, 1)
#     ğ“ˆ = sqrt.(diag(Mâ»Â¹))
#     t = 0
#     while true
#         qâ‚€ = randn(K) .* ğ“ˆ
#         qâ€², pâ€² = hmc_nc_1iter(f_gradlogÏ€, f_logÏ€q, Ïµ, L, M, Mâ»Â¹, qâ‚€, y, ÏƒÂ²)
#         t += 1
#         qâ€² != qâ‚€ && return qâ€², t
#     end
#     return qâ€², t
# end
# # Initialization tests -- guarantee a decent starting point
# qâ‚€, t = initial_nc(gradlogÏ€_nc, logÏ€q_nc, Ïµ, L, M, Mâ»Â¹, y, ÏƒÂ²)

# Data
y = Float64[28, 8, -3, 7, -1, 1, 18, 12];
Ïƒ = Float64[15, 10, 16, 11, 9, 11, 10, 18];
ÏƒÂ² = abs2.(Ïƒ);

# Integration time and stepsize. Interestingly, after re-parameterization,
# Ïµ = 0.2, L = 5 gives a reasonable acceptance rate. Ïµ = 0.1, L = 10 results in acceptance
# rate of â‰¥ 0.99, which indicates the integration could take larger steps.
Ïµ = 0.2
L = 5
N = 1000

# Euclidean-Gaussian metric
scale = 15;
Mâ»Â¹ = diagm(ones(10)); Mâ»Â¹[9, 9] = scale^2;
M = diagm(1 ./ diag(Mâ»Â¹));

# Single step, random initialization
qâ‚€, pâ‚€ = hmc_nc_transition(gradlogÏ€_nc, logÏ€q_nc, Ïµ, L, M, Mâ»Â¹, y, ÏƒÂ²)
# Single step, fixed initialization
@timev q, p = hmc_nc_transition(gradlogÏ€_nc, logÏ€q_nc, Ïµ, L, M, Mâ»Â¹, qâ‚€, y, ÏƒÂ²)
# Fixed initialization
@timev Î¸, n, t = hmc_nc(gradlogÏ€_nc, logÏ€q_nc, N, Ïµ, L, M, Mâ»Â¹, qâ‚€, y, ÏƒÂ²);

# Auto-initialization
@timev Î¸, n, t = hmc_nc(gradlogÏ€_nc, logÏ€q_nc, N, Ïµ, L, M, Mâ»Â¹, y, ÏƒÂ²);
acprob = n / t
mean(Î¸, dims=2)

Ï‘ = originalparam(Î¸);
mean(Ï‘, dims=2)

# Multiple chains
@timev chains = [hmc_nc(gradlogÏ€_nc, logÏ€q_nc, N, Ïµ, L, M, Mâ»Â¹, y, ÏƒÂ²) for _ = 1:4];
acprob = getindex.(chains, 2) ./ getindex.(chains, 3)
Î¸ = cat(first.(chains)..., dims=3);
mean(Î¸, dims=(2,3))

Ï‘ = originalparam(Î¸);
mean(Ï‘, dims=(2, 3))
# Convergence check
neff(Ï‘)
Rhat(Ï‘)

# Trajectory tests
pâ‚€ = momentumrand_M(M)
q, p = symplecticintegrate(gradlogÏ€_nc, qâ‚€, pâ‚€, Mâ»Â¹, Ïµ, L, y, ÏƒÂ²)
#

# # Experiment with NamedTuple -- no change in allocations or speed
# ğ’¹ = (; :y => y, :ÏƒÂ² => ÏƒÂ²)
# logÏ€q_nc(q, ğ’¹::NamedTuple) = logÏ€q_nc(q[1:8], q[9], q[10], ğ’¹.y, ğ’¹.ÏƒÂ²)
# @timev logÏ€q_nc(qâ‚€, y, ÏƒÂ²)
# @timev logÏ€q_nc(qâ‚€, ğ’¹)

function originalparam(Î¸::Vector{T}) where {T<:Real}
    K = length(Î¸)
    Ï„ = exp(Î¸[K])
    Î¼ = Î¸[K - 1]
    Ï‘ = Vector{promote_type(T, Float64)}(undef, K)
    for k = 1:(K - 2)
        Ï‘[k] = Î¼ + Ï„ * Î¸[k]
    end
    Ï‘[K - 1] = Î¼
    Ï‘[K] = Ï„
    Ï‘
end

function originalparam(Î¸::Matrix{T}) where {T<:Real}
    K, S = size(Î¸)
    Ï‘ = Matrix{promote_type(T, Float64)}(undef, K, S)
    for s âˆˆ axes(Î¸, 2)
        Ï„ = exp(Î¸[K, s])
        Î¼ = Î¸[K - 1, s]
        for k = 1:(K - 2)
            Ï‘[k, s] = Î¼ + Ï„ * Î¸[k, s]
        end
        Ï‘[K - 1, s] = Î¼
        Ï‘[K, s] = Ï„
    end
    Ï‘
end
originalparam(Î¸::Array{T, 3}) where {T<:Real} = mapslices(originalparam, Î¸, dims=(1, 2))


################################################################
# Functions to support formal version, other than those provided above
function symplecticintegrate(f_gradlogÏ€, qâ‚€, pâ‚€, Mâ»Â¹, Ïµ, L)
    pâ€² = pâ‚€ + 0.5Ïµ * f_gradlogÏ€(qâ‚€)
    qâ€² = qâ‚€ + Ïµ * Mâ»Â¹ * pâ€²
    for l = 2:L
        pâ€² = pâ€² + Ïµ * f_gradlogÏ€(qâ€²)
        qâ€² = qâ€² + Ïµ * Mâ»Â¹ * pâ€²
    end
    pâ€² = pâ€² + 0.5Ïµ * f_gradlogÏ€(qâ€²)
    qâ€², -pâ€²
end

################ Thinking in terms of the Hamiltonian
H(f_logÏ€q, q, p, Mâ»Â¹) = -f_logÏ€q(q) - logÏ€p(p, Mâ»Â¹)
E_pq(f_logÏ€q, q, E) = E + f_logÏ€q(q)
logÏ€p(f_logÏ€q, q, E) = E_pq(f_logÏ€q, q, E)

function transition(f_logÏ€q, f_gradlogÏ€, qâ‚€, pâ‚€, M, Mâ»Â¹, Ïµ, L)
    qâ€², pâ€² = symplecticintegrate(f_gradlogÏ€, qâ‚€, pâ‚€, Mâ»Â¹, Ïµ, L)
    r = exp(-H(f_logÏ€q, qâ€², pâ€², Mâ»Â¹) + H(f_logÏ€q, qâ‚€, pâ‚€, Mâ»Â¹))
    u = rand()
    u < r ? (qâ€², pâ€²) : (qâ‚€, pâ‚€)
end
transition(f_logÏ€q, f_gradlogÏ€, qâ‚€, M, Mâ»Â¹, Ïµ, L) =
    transition(f_logÏ€q, f_gradlogÏ€, qâ‚€, momentumrand_M(M), M, Mâ»Â¹, Ïµ, L)
transition(f_logÏ€q, f_gradlogÏ€, M, Mâ»Â¹, Ïµ, L) =
    transition(f_logÏ€q, f_gradlogÏ€, targetrand(Mâ»Â¹), momentumrand_M(M), M, Mâ»Â¹, Ïµ, L)

function hmc(f_logÏ€q, f_gradlogÏ€, qâ‚€, M, Mâ»Â¹, Ïµ, L, N)
    Î¸ = Matrix{Float64}(undef, size(M, 1), N)
    E = Vector{Float64}(undef, N)
    n, t = 0, 0
    while n < N
        pâ‚€ = momentumrand_M(M)
        qâ€², pâ€² = symplecticintegrate(f_gradlogÏ€, qâ‚€, pâ‚€, Mâ»Â¹, Ïµ, L)
        Eâ‚€ = H(f_logÏ€q, qâ‚€, pâ‚€, Mâ»Â¹)
        Eâ€² = H(f_logÏ€q, qâ€², pâ€², Mâ»Â¹)
        r = exp(-Eâ€² + Eâ‚€)
        if rand() < r
            n += 1
            Î¸[:, n] = qâ€²
            qâ‚€ = qâ€²
            E[n] = Eâ€²
        end
        t += 1
    end
    Î¸, E, n, t
end
hmc(f_logÏ€q, f_gradlogÏ€, M, Mâ»Â¹, Ïµ, L, N) =
    hmc(f_logÏ€q, f_gradlogÏ€, targetrand(Mâ»Â¹), M, Mâ»Â¹, Ïµ, L, N)

function ebfmi(E::Vector{T}) where {T<:Real}
    N = length(E)
    s = zero(T)
    # As written, this has _clear_ potential for numerical stability issues
    for n = 2:N
        s += abs2(E[n] - E[n - 1])
    end
    s / (N * var(E, corrected=false))
end

function microcanonicalenergy(f_logÏ€q, q, E)
    Eq = -f_logÏ€q(q)
    Epq = E - Epq
    Epq, Eq
end

function microcanonicalenergy(f_logÏ€q, Q::Matrix{T}, E::Vector{T}) where {T<:Real}
    N = length(E)
    Eq = Vector{Float64}(undef, N)
    Epq = Vector{Float64}(undef, N)
    for (n, q) âˆˆ enumerate(eachcol(Q))
        Eq[n] = -f_logÏ€q(q)
        Epq[n] = E[n] - Eq[n]
    end
    Epq, Eq
end

# Guaranteed initialization for models in which small Ï„ causes sticking
# (i.e. for centered parameterizations)
function initialize(f_logÏ€q, f_gradlogÏ€, M, Mâ»Â¹, Ïµ, L)
    t = 0
    while true
        qâ‚€, pâ‚€ = targetrand(Mâ»Â¹), momentumrand_M(M)
        qâ€², pâ€² = transition(f_logÏ€q, f_gradlogÏ€, qâ‚€, pâ‚€, M, Mâ»Â¹, Ïµ, L)
        t += 1
        qâ€² != qâ‚€ && return qâ€², pâ€², t
    end
    return qâ€², pâ€², t
end

# Data
y = Float64[28, 8, -3, 7, -1, 1, 18, 12];
Ïƒ = Float64[15, 10, 16, 11, 9, 11, 10, 18];
ÏƒÂ² = abs2.(Ïƒ);

# Integration time and stepsize.
Ïµ = 0.2
L = 5
N = 1000

# Euclidean-Gaussian metric
scale = 15;
Mâ»Â¹ = diagm(ones(10)); Mâ»Â¹[9, 9] = scale^2;
M = diagm(1 ./ diag(Mâ»Â¹));

f_logÏ€ = let y = y, ÏƒÂ² = ÏƒÂ²
    q -> logÏ€q_nc(q, y, ÏƒÂ²)
end;
f_gradlogÏ€ = let y = y, ÏƒÂ² = ÏƒÂ²
    q -> gradlogÏ€_nc(q, y, ÏƒÂ²)
end;

@benchmark transition(f_logÏ€, f_gradlogÏ€, M, Mâ»Â¹, Ïµ, L)
@timev Î¸, E, n, t = hmc(f_logÏ€, f_gradlogÏ€, M, Mâ»Â¹, Ïµ, L, N);
@benchmark hmc(f_logÏ€, f_gradlogÏ€, M, Mâ»Â¹, Ïµ, L, N)

ebfmi(E)

Epq, Eq = microcanonicalenergy(f_logÏ€, Î¸, E);
pâ‚• = histogram([Epq Eq] .- [mean(Epq) mean(Eq)], labels=["E_pq â‰¡ Ï€(E|q)" "E_q â‰¡ Ï€(E)"], alpha=0.5,
               annotations=((0.1, 1.0), text("E-BFMI = $(round(ebfmi(E), digits=3))", 8)),
               xlabel="E - <E>");
savefig(pâ‚•, joinpath(pwd(), "energy.pdf"))

# Multiple chains
@timev chains = [hmc(f_logÏ€, f_gradlogÏ€, M, Mâ»Â¹, Ïµ, L, N) for _ = 1:4];
acprob = getindex.(chains, 3) ./ getindex.(chains, 4)
Î¸ = cat(first.(chains)..., dims=3);
mean(Î¸, dims=(2,3))
E = hcat(getindex.(chains, 2)...);

Ï‘ = originalparam(Î¸);
mean(Ï‘, dims=(2, 3))
# Convergence check
neff(Ï‘)
Rhat(Ï‘)

ps = map(1:4) do k
    Epq, Eq = microcanonicalenergy(f_logÏ€, Î¸[:, :, k], E[:, k])
    pâ‚• = histogram([Epq Eq] .- [mean(Epq) mean(Eq)], labels=["E_pq â‰¡ Ï€(E|q)" "E_q â‰¡ Ï€(E)"],
                   alpha=0.5,
                   annotations=((0.1, 1.0), text("E-BFMI = $(round(ebfmi(E[:, k]), digits=3))", 8)),
                   xlabel="E - <E>")
    pâ‚•
end;
pâ‚• = plot(ps...);
savefig(pâ‚•, joinpath(pwd(), "energy_$(N_chains)chain.pdf"))

#### Centered parameterization -- might get stuck due to small Ï„
Ïµ = 0.1
L = 10
N = 1000
N_chains = 4

scale = 15;
M = diagm(fill(1 / scale^2, 10)); M[10, 10] = 1;
Mâ»Â¹ = diagm(1 ./ diag(M));

f_logÏ€_c = let y = y, ÏƒÂ² = ÏƒÂ²
    q -> logÏ€q(q, y, ÏƒÂ²)
end;
f_gradlogÏ€_c = let y = y, ÏƒÂ² = ÏƒÂ²
    q -> gradlogÏ€(q, y, ÏƒÂ²)
end;


qq, pp = transition(f_logÏ€_c, f_gradlogÏ€_c, M, Mâ»Â¹, Ïµ, L)
# Typically, centered parameterization necessitates a non-small Ï„ starting point
qáµ¢, páµ¢, táµ¢ = initialize(f_logÏ€_c, f_gradlogÏ€_c, M, Mâ»Â¹, Ïµ, L)
@timev Î¸, E, n, t = hmc(f_logÏ€_c, f_gradlogÏ€_c, qáµ¢, M, Mâ»Â¹, Ïµ, L, N);


Epq, Eq = microcanonicalenergy(f_logÏ€_c, Î¸, E);
pâ‚• = histogram([Epq Eq] .- [mean(Epq) mean(Eq)], labels=["E_pq â‰¡ Ï€(E|q)" "E_q â‰¡ Ï€(E)"], alpha=0.5,
               annotations=((0.1, 1.0), text("E-BFMI = $(round(ebfmi(E), digits=3))", 8)),
               xlabel="E - <E>");
savefig(pâ‚•, joinpath(pwd(), "energy_c.pdf"))

# Multiple chains
@timev chains = [hmc(f_logÏ€_c, f_gradlogÏ€_c,
                     first(initialize(f_logÏ€_c, f_gradlogÏ€_c, M, Mâ»Â¹, Ïµ, L)), # qáµ¢'s
                     M, Mâ»Â¹, Ïµ, L, N) for _ = 1:N_chains];
acprob = getindex.(chains, 3) ./ getindex.(chains, N_chains)
Î¸ = cat(first.(chains)..., dims=3);
mean(Î¸, dims=(2,3))
E = hcat(getindex.(chains, 2)...);

# Convergence check
neff(Î¸)
Rhat(Î¸)

ps = map(1:4) do k
    Epq, Eq = microcanonicalenergy(f_logÏ€_c, Î¸[:, :, k], E[:, k])
    pâ‚• = histogram([Epq Eq] .- [mean(Epq) mean(Eq)], labels=["E_pq â‰¡ Ï€(E|q)" "E_q â‰¡ Ï€(E)"],
                   alpha=0.5,
                   annotations=((0.1, 1.0), text("E-BFMI = $(round(ebfmi(E[:, k]), digits=3))", 8)),
                   xlabel="E - <E>")
    pâ‚•
end;
pâ‚• = plot(ps...);
savefig(pâ‚•, joinpath(pwd(), "energy_c_$(N_chains)chain.pdf"))
############################################################################################
#### MCMC diagnostics: potential scale reduction and effective sample size
# Simulations arranged on dimension 2, chains on dimension 3
# function withinchainmean(chain::AbstractMatrix)
#     ÏˆÌ„ = mean(chain, dims=2)
# end
function betweenchain(Ï‘::Array{T, 3}) where {T<:Real}
    B = size(Ï‘, 2) .* var(dropdims(mean(Ï‘, dims=2), dims=2), dims=2)
end

function withinchain(Ï‘::Array{T, 3}) where {T<:Real}
    W = mean(dropdims(var(Ï‘, dims=2), dims=2), dims=2)
end

function vhat(Ïˆ::AbstractMatrix{T}) where {T<:Real}
    n = size(Ïˆ, 1)
    B = n * var(mean(Ïˆ, dims=1))
    W = mean(var(Ïˆ, dims=1))
    vÌ‚âº = ((n - 1) / n) * W + (1 / n) * B
end


function vhat(Ï‘::Array{T, 3}) where {T<:Real}
    n = size(Ï‘, 2)
    B = n .* var(dropdims(mean(Ï‘, dims=2), dims=2), dims=2)
    W = mean(dropdims(var(Ï‘, dims=2), dims=2), dims=2)
    vÌ‚âº = ((n - 1) / n) .* W .+ (1 / n) .* B
end

function Rhat(Ïˆ::AbstractMatrix{T}) where {T<:Real}
    n = size(Ïˆ, 1)
    B = n * var(mean(Ïˆ, dims=1))
    W = mean(var(Ïˆ, dims=1))
    vÌ‚âº = ((n - 1) / n) * W + (1 / n) * B
    RÌ‚ = âˆš(vÌ‚âº / W)
end

function Rhat(Ï‘::Array{T, 3}) where {T<:Real}
    n = size(Ï‘, 2)
    B = n .* var(dropdims(mean(Ï‘, dims=2), dims=2), dims=2)
    W = mean(dropdims(var(Ï‘, dims=2), dims=2), dims=2)
    vÌ‚âº = ((n - 1) / n) .* W .+ (1 / n) .* B
    RÌ‚ = sqrt.(vÌ‚âº ./ W)
end


Ï‘ = Î¸[:, (N >> 1 + 1):N, :]; #view(Î¸, :, (N >> 1 + 1):N, :);
L = size(Ï‘, 2)
n = L >> 1
Ï‘â‚‚ = [view(Ï‘, :, 1:n, :);;; view(Ï‘, :, (n + 1):L, :)];

Ïˆ = Ï‘â‚‚[1, :, :];

B = betweenchain(Ï‘)
W = withinchain(Ï‘)
vÌ‚âº = ((L - 1) / L) .* W .+ (1 / L) .* B
RÌ‚ = sqrt.(vÌ‚âº ./ W)

Rhat(Ï‘)
Rhat(Ï‘â‚‚)

function variogram(Ïˆ::AbstractMatrix{T}, t::Int) where {T<:Real}
    n, m = size(Ïˆ)
    s = zero(T)
    for j âˆˆ axes(Ïˆ, 2)
        for i = (t + 1):n
            s += abs2(Ïˆ[i, j] - Ïˆ[i - t, j])
        end
    end
    return s / (m * (n - t))
end

autocor(Ïˆ::AbstractMatrix{T}, t::Int) where {T<:Real} = one(T) - variogram(Ïˆ, t) / 2vhat(Ïˆ)

function findautolag(Ïˆ::AbstractMatrix{T}) where {T<:Real}
    n = size(Ïˆ, 1)
    for t = 1:2:(n - 3)
        ÏÌ‚â‚œâ‚Šâ‚ = autocor(Ïˆ, t)
        ÏÌ‚â‚œâ‚Šâ‚‚ = autocor(Ïˆ, t + 1)
        ÏÌ‚â‚œâ‚Šâ‚ + ÏÌ‚â‚œâ‚Šâ‚‚ < 0 && return t
    end
    return n - 3
end

function neff(Ïˆ::AbstractMatrix{T}) where {T<:Real}
    n, m = size(Ïˆ)
    # t = findautolag(Ïˆ)
    Î£Ï = zero(T)
    for t = 1:2:(n - 3)
        ÏÌ‚â‚œâ‚Šâ‚ = autocor(Ïˆ, t)
        ÏÌ‚â‚œâ‚Šâ‚‚ = autocor(Ïˆ, t + 1)
        s = ÏÌ‚â‚œâ‚Šâ‚ + ÏÌ‚â‚œâ‚Šâ‚‚
        s < 0 && break
        Î£Ï += s
        # s < 0 ? break : (Î£Ï += s)
    end
    nÌ‚â‚‘ = n * m / (one(T) + 2Î£Ï)
end

neff(Ï‘::Array{T, 3}) where {T<:Real} = mapslices(neff, Ï‘, dims=(2, 3))
neff(Ï‘)
Rhat(Ï‘)

variogram(Ïˆ, 1)
findautolag(Ïˆ)
@benchmark neff(Ïˆ)
mapslices(neff, cÌƒ, dims=(2, 3))
mapslices(Rhat, cÌƒ, dims=(2, 3))

################################################################
# Import Turing and DynamicHMC.
using DynamicHMC, Turing

# Model definition.
@model function gdemo(x, y)
    sÂ² ~ InverseGamma(2, 3)
    m ~ Normal(0, sqrt(sÂ²))
    x ~ Normal(m, sqrt(sÂ²))
    y ~ Normal(m, sqrt(sÂ²))
end

# Pull 2,000 samples using DynamicNUTS.
chn = sample(gdemo(1.5, 2.0), DynamicNUTS(), 2000)

#### The classic 8-schools
@model function sch8(y, Ïƒ)
    Ï„ ~ Uniform(0, 225)
    Î¼ ~ Uniform(-225, 225)
    k = length(y)
    Î± ~ filldist(Normal(Î¼, Ï„), k)
    y .~ Normal.(Î±, Ïƒ)
end
@timev chn2 = sample(sch8(y, Ïƒ), DynamicNUTS(), MCMCThreads(), 1000, 4)
Î¸2 = chn2.value.data
mean(Î¸2, dims=1)
chn2[:Î±]
chn3 = sample(sch8(y, Ïƒ), HMC(Ïµ, L), 1000)

############################################################################################
using LinearAlgebra, Statistics
############################################################################################
#General utilities
function inversediagdet(Mâ»Â¹::Matrix{T}) where {T<:Real}
    s = one(T)
    for j âˆˆ axes(Mâ»Â¹, 1)
        s *= one(T) / Mâ»Â¹[j, j]
    end
    s
end

function weightedss(p, Mâ»Â¹::Matrix{T}) where {T<:Real}
    s = zero(T)
    for j âˆˆ axes(Mâ»Â¹, 1)
        s += Mâ»Â¹[j, j] * abs2(p[j])
        # or:
        # pâ±¼ = p[j]
        # s += pâ±¼ * pâ±¼ * Mâ»Â¹[j, j]
    end
    s
end

function logÏ€p(p, Mâ»Â¹)
    # s = length(p) * log(2Ï€)
    # s += log(inversediagdet(Mâ»Â¹))
    # s += weightedss(p, Mâ»Â¹)
    # s *= -0.5
    # s
    # # Or, simpler:
    -0.5 * (length(p) * log(2Ï€) + log(inversediagdet(Mâ»Â¹)) + weightedss(p, Mâ»Â¹))
    # # unnormalized density
    # -0.5weightedss(p, Mâ»Â¹) -0.5log(inversediagdet(Mâ»Â¹))
    # -0.5weightedss(p, Mâ»Â¹)
end

# Using the mass matrix directly
function momentumrand_M(M)
    p = randn(size(M, 1))
    @inbounds for j âˆˆ axes(M, 1)
        p[j] *= âˆš(M[j, j])
    end
    p
end

# Using the lower triangular of the cholesky, AAáµ€ = M
function momentumrand_A(A)
    p = randn(size(A, 1))
    @inbounds for j âˆˆ axes(A, 1)
        p[j] *= A[j, j]
    end
    p
end

# Using the inverse mass matrix directly
function momentumrand_Mâ»Â¹(Mâ»Â¹)
    p = randn(size(Mâ»Â¹, 1))
    @inbounds for j âˆˆ axes(Mâ»Â¹, 1)
        p[j] *= inv(âˆš(Mâ»Â¹[j, j]))
    end
    p
end

function targetrand(Mâ»Â¹::Matrix{T}) where {T<:Real}
    K = size(Mâ»Â¹, 1)
    qâ‚€ = randn(K)
    for k âˆˆ eachindex(qâ‚€)
        qâ‚€[k] *= âˆš(Mâ»Â¹[k, k])
    end
    qâ‚€
end
############################################################################################
# Static HMC implementation:
# This is a high-level implementation, not as efficient as I would normally write.
# Much, so much in-place mutation can be used to minimize memory usage...
################################################################
# Functions to support formal version, other than those provided above
function symplecticintegrate(f_gradlogÏ€, qâ‚€, pâ‚€, Mâ»Â¹, Ïµ, L)
    pâ€² = pâ‚€ + 0.5Ïµ * f_gradlogÏ€(qâ‚€)
    qâ€² = qâ‚€ + Ïµ * Mâ»Â¹ * pâ€²
    for l = 2:L
        pâ€² = pâ€² + Ïµ * f_gradlogÏ€(qâ€²)
        qâ€² = qâ€² + Ïµ * Mâ»Â¹ * pâ€²
    end
    pâ€² = pâ€² + 0.5Ïµ * f_gradlogÏ€(qâ€²)
    qâ€², -pâ€²
end

################ Thinking in terms of the Hamiltonian
H(f_logÏ€q, q, p, Mâ»Â¹) = -f_logÏ€q(q) - logÏ€p(p, Mâ»Â¹)
E_pq(f_logÏ€q, q, E) = E + f_logÏ€q(q)
logÏ€p(f_logÏ€q, q, E) = E_pq(f_logÏ€q, q, E)

function transition(f_logÏ€q, f_gradlogÏ€, qâ‚€, pâ‚€, M, Mâ»Â¹, Ïµ, L)
    qâ€², pâ€² = symplecticintegrate(f_gradlogÏ€, qâ‚€, pâ‚€, Mâ»Â¹, Ïµ, L)
    r = exp(-H(f_logÏ€q, qâ€², pâ€², Mâ»Â¹) + H(f_logÏ€q, qâ‚€, pâ‚€, Mâ»Â¹))
    u = rand()
    u < r ? (qâ€², pâ€²) : (qâ‚€, pâ‚€)
end
transition(f_logÏ€q, f_gradlogÏ€, qâ‚€, M, Mâ»Â¹, Ïµ, L) =
    transition(f_logÏ€q, f_gradlogÏ€, qâ‚€, momentumrand_M(M), M, Mâ»Â¹, Ïµ, L)
transition(f_logÏ€q, f_gradlogÏ€, M, Mâ»Â¹, Ïµ, L) =
    transition(f_logÏ€q, f_gradlogÏ€, targetrand(Mâ»Â¹), momentumrand_M(M), M, Mâ»Â¹, Ïµ, L)

function hmc(f_logÏ€q, f_gradlogÏ€, qâ‚€, M, Mâ»Â¹, Ïµ, L, N)
    Î¸ = Matrix{Float64}(undef, size(M, 1), N)
    E = Vector{Float64}(undef, N)
    n, t = 0, 0
    while n < N
        pâ‚€ = momentumrand_M(M)
        qâ€², pâ€² = symplecticintegrate(f_gradlogÏ€, qâ‚€, pâ‚€, Mâ»Â¹, Ïµ, L)
        Eâ‚€ = H(f_logÏ€q, qâ‚€, pâ‚€, Mâ»Â¹)
        Eâ€² = H(f_logÏ€q, qâ€², pâ€², Mâ»Â¹)
        r = exp(-Eâ€² + Eâ‚€)
        if rand() < r
            n += 1
            Î¸[:, n] = qâ€²
            qâ‚€ = qâ€²
            E[n] = Eâ€²
        end
        t += 1
    end
    Î¸, E, n, t
end
hmc(f_logÏ€q, f_gradlogÏ€, M, Mâ»Â¹, Ïµ, L, N) =
    hmc(f_logÏ€q, f_gradlogÏ€, targetrand(Mâ»Â¹), M, Mâ»Â¹, Ïµ, L, N)

function ebfmi(E::Vector{T}) where {T<:Real}
    N = length(E)
    s = zero(T)
    # As written, this has _clear_ potential for numerical stability issues
    for n = 2:N
        s += abs2(E[n] - E[n - 1])
    end
    s / (N * var(E, corrected=false))
end

function microcanonicalenergy(f_logÏ€q, q, E)
    Eq = -f_logÏ€q(q)
    Epq = E - Epq
    Epq, Eq
end

function microcanonicalenergy(f_logÏ€q, Q::Matrix{T}, E::Vector{T}) where {T<:Real}
    N = length(E)
    Eq = Vector{Float64}(undef, N)
    Epq = Vector{Float64}(undef, N)
    for (n, q) âˆˆ enumerate(eachcol(Q))
        Eq[n] = -f_logÏ€q(q)
        Epq[n] = E[n] - Eq[n]
    end
    Epq, Eq
end

# Guaranteed initialization for models in which small Ï„ causes sticking
# (i.e. for centered parameterizations)
function initialize(f_logÏ€q, f_gradlogÏ€, M, Mâ»Â¹, Ïµ, L)
    t = 0
    while true
        qâ‚€, pâ‚€ = targetrand(Mâ»Â¹), momentumrand_M(M)
        qâ€², pâ€² = transition(f_logÏ€q, f_gradlogÏ€, qâ‚€, pâ‚€, M, Mâ»Â¹, Ïµ, L)
        t += 1
        qâ€² != qâ‚€ && return qâ€², pâ€², t
    end
    return qâ€², pâ€², t
end

############################################################################################
# Centered parameterization:
# yâ±¼ ~ N(Î±â±¼, Ïƒâ±¼Â²)
# Î±â±¼ ~ N(Î¼, Ï„Â²)
# Î¼ ~ U(-âˆ, âˆ)
# Ï„ ~ U(0, âˆ)
# Ï„ transformed to logÏ„ to place on unconstrained space

function gradlogÏ€(Î±, Î¼, logÏ„, y, ÏƒÂ²)
    Ï„Â² = abs2(exp(logÏ„))
    # âˆ‡l = @. - (Î± - y) / ÏƒÂ² - (Î± - Î¼) / Ï„Â²
    âˆ‡l = .- (Î± .- y) ./ ÏƒÂ² .- (Î± .- Î¼) ./ Ï„Â²
    dÏ€dÎ¼ = 0.
        for j âˆˆ axes(Î±, 1)
            dÏ€dÎ¼ -= (Î¼ - Î±[j]) / Ï„Â² # -
        end
    dÏ€dlogÏ„ = - length(Î±) + 1.
        for j âˆˆ axes(Î±, 1)
            dÏ€dlogÏ„ += abs2(Î±[j] - Î¼) / Ï„Â²
        end
    [âˆ‡l; dÏ€dÎ¼; dÏ€dlogÏ„]
end
gradlogÏ€(q, y, ÏƒÂ²) = gradlogÏ€(q[1:8], q[9], q[10], y, ÏƒÂ²)

function logÏ€q(Î±, Î¼, logÏ„, y, ÏƒÂ²)
    Ï„Â² = abs2(exp(logÏ„))
    # s = 0
    # s += logÏ„
    s = logÏ„
    for j âˆˆ axes(Î±, 1)
        s += -log(âˆš(2Ï€ * ÏƒÂ²[j])) - 0.5abs2(y[j] - Î±[j]) / ÏƒÂ²[j] - 0.5abs2(Î±[j] - Î¼) / Ï„Â²
        # s += - 0.5log(2Ï€ * ÏƒÂ²[j]) - 0.5abs2(y[j] - Î±[j]) / ÏƒÂ²[j] - 0.5abs2(Î±[j] - Î¼) / Ï„Â²
    end
    s -= length(Î±) * log(âˆš(2Ï€ * Ï„Â²))
    # s -= length(Î±) * 0.5log(2Ï€ * Ï„Â²)
    s
    # # unnormalized density
    # Ï„Â² = abs2(exp(logÏ„))
    # s = logÏ„
    # for j âˆˆ axes(Î±, 1)
    #     s += -0.5abs2(y[j] - Î±[j]) / ÏƒÂ²[j] - 0.5abs2(Î±[j] - Î¼) / Ï„Â²
    # end
    # s += -0.5log(2Ï€ * Ï„Â²)
    # s
end
logÏ€q(q, y, ÏƒÂ²) = logÏ€q(q[1:8], q[9], q[10], y, ÏƒÂ²)
############################################################################################
#### 2022-02-14: non-centered parameterization
# yâ±¼ ~ N(Î¼ + Ï„ * Î·â±¼, Ïƒâ±¼Â²)
# Î·â±¼ ~ N(0, 1)
# Î¼ ~ U(-âˆ, âˆ)
# Ï„ ~ U(0, âˆ)
# Ï„ transformed to logÏ„ to place on unconstrained space

# Clearly not the most efficient approach -- the proper way is to compute a vector, the
# elements of which are (y[j] - Î¼ - Ï„ * Î·[j]). Then, compute dÏ€dÎ¼ and dÏ€dlogÏ„, then
# mutate in place to complete the computation of âˆ‡l.
function gradlogÏ€_nc(Î·, Î¼, logÏ„, y, ÏƒÂ²)
    Ï„ = exp(logÏ„)
    âˆ‡l = Vector{Float64}(undef, length(Î·))
    for j âˆˆ eachindex(Î·, y, ÏƒÂ²)
        âˆ‡l[j] = (y[j] - Î¼ - Ï„ * Î·[j]) * Ï„ / ÏƒÂ²[j] - Î·[j]
    end
    dÏ€dÎ¼ = 0.0
    for j âˆˆ eachindex(Î·, y, ÏƒÂ²)
        dÏ€dÎ¼ += (y[j] - Î¼ - Ï„ * Î·[j]) / ÏƒÂ²[j]
    end
    dÏ€dlogÏ„ = 1.0
    for j âˆˆ eachindex(Î·, y, ÏƒÂ²)
        dÏ€dlogÏ„ += Ï„ * (y[j] - Î¼ - Ï„ * Î·[j]) * Î·[j] / ÏƒÂ²[j]
    end
    [âˆ‡l; dÏ€dÎ¼; dÏ€dlogÏ„]
end
gradlogÏ€_nc(q, y, ÏƒÂ²) = gradlogÏ€_nc(q[1:8], q[9], q[10], y, ÏƒÂ²)

function logÏ€q_nc(Î·, Î¼, logÏ„, y, ÏƒÂ²)
    Ï„ = exp(logÏ„)
    s = logÏ„
    for j âˆˆ eachindex(Î·, y, ÏƒÂ²)
        s += -0.5(log(2Ï€ * ÏƒÂ²[j]) + (1.0 / ÏƒÂ²[j]) * abs2(y[j] - Î¼ - Ï„ * Î·[j]) + log(2Ï€) + abs2(Î·[j]))
    end
    s
end
logÏ€q_nc(q, y, ÏƒÂ²) = logÏ€q_nc(q[1:8], q[9], q[10], y, ÏƒÂ²)

#### Utilities for conversion to original parameterization
function originalparam(Î¸::Vector{T}) where {T<:Real}
    K = length(Î¸)
    Ï„ = exp(Î¸[K])
    Î¼ = Î¸[K - 1]
    Ï‘ = Vector{promote_type(T, Float64)}(undef, K)
    for k = 1:(K - 2)
        Ï‘[k] = Î¼ + Ï„ * Î¸[k]
    end
    Ï‘[K - 1] = Î¼
    Ï‘[K] = Ï„
    Ï‘
end

function originalparam(Î¸::Matrix{T}) where {T<:Real}
    K, S = size(Î¸)
    Ï‘ = Matrix{promote_type(T, Float64)}(undef, K, S)
    for s âˆˆ axes(Î¸, 2)
        Ï„ = exp(Î¸[K, s])
        Î¼ = Î¸[K - 1, s]
        for k = 1:(K - 2)
            Ï‘[k, s] = Î¼ + Ï„ * Î¸[k, s]
        end
        Ï‘[K - 1, s] = Î¼
        Ï‘[K, s] = Ï„
    end
    Ï‘
end
originalparam(Î¸::Array{T, 3}) where {T<:Real} = mapslices(originalparam, Î¸, dims=(1, 2))

############################################################################################
#### MCMC diagnostics: potential scale reduction and effective sample size
# Simulations arranged on dimension 2, chains on dimension 3
# function withinchainmean(chain::AbstractMatrix)
#     ÏˆÌ„ = mean(chain, dims=2)
# end
function betweenchain(Ï‘::Array{T, 3}) where {T<:Real}
    B = size(Ï‘, 2) .* var(dropdims(mean(Ï‘, dims=2), dims=2), dims=2)
end

function withinchain(Ï‘::Array{T, 3}) where {T<:Real}
    W = mean(dropdims(var(Ï‘, dims=2), dims=2), dims=2)
end

function vhat(Ïˆ::AbstractMatrix{T}) where {T<:Real}
    n = size(Ïˆ, 1)
    B = n * var(mean(Ïˆ, dims=1))
    W = mean(var(Ïˆ, dims=1))
    vÌ‚âº = ((n - 1) / n) * W + (1 / n) * B
end


function vhat(Ï‘::Array{T, 3}) where {T<:Real}
    n = size(Ï‘, 2)
    B = n .* var(dropdims(mean(Ï‘, dims=2), dims=2), dims=2)
    W = mean(dropdims(var(Ï‘, dims=2), dims=2), dims=2)
    vÌ‚âº = ((n - 1) / n) .* W .+ (1 / n) .* B
end

function Rhat(Ïˆ::AbstractMatrix{T}) where {T<:Real}
    n = size(Ïˆ, 1)
    B = n * var(mean(Ïˆ, dims=1))
    W = mean(var(Ïˆ, dims=1))
    vÌ‚âº = ((n - 1) / n) * W + (1 / n) * B
    RÌ‚ = âˆš(vÌ‚âº / W)
end

function Rhat(Ï‘::Array{T, 3}) where {T<:Real}
    n = size(Ï‘, 2)
    B = n .* var(dropdims(mean(Ï‘, dims=2), dims=2), dims=2)
    W = mean(dropdims(var(Ï‘, dims=2), dims=2), dims=2)
    vÌ‚âº = ((n - 1) / n) .* W .+ (1 / n) .* B
    RÌ‚ = sqrt.(vÌ‚âº ./ W)
end

function variogram(Ïˆ::AbstractMatrix{T}, t::Int) where {T<:Real}
    n, m = size(Ïˆ)
    s = zero(T)
    for j âˆˆ axes(Ïˆ, 2)
        for i = (t + 1):n
            s += abs2(Ïˆ[i, j] - Ïˆ[i - t, j])
        end
    end
    return s / (m * (n - t))
end

autocor(Ïˆ::AbstractMatrix{T}, t::Int) where {T<:Real} = one(T) - variogram(Ïˆ, t) / 2vhat(Ïˆ)

function findautolag(Ïˆ::AbstractMatrix{T}) where {T<:Real}
    n = size(Ïˆ, 1)
    for t = 1:2:(n - 3)
        ÏÌ‚â‚œâ‚Šâ‚ = autocor(Ïˆ, t)
        ÏÌ‚â‚œâ‚Šâ‚‚ = autocor(Ïˆ, t + 1)
        ÏÌ‚â‚œâ‚Šâ‚ + ÏÌ‚â‚œâ‚Šâ‚‚ < 0 && return t
    end
    return n - 3
end

function neff(Ïˆ::AbstractMatrix{T}) where {T<:Real}
    n, m = size(Ïˆ)
    # t = findautolag(Ïˆ)
    Î£Ï = zero(T)
    for t = 1:2:(n - 3)
        ÏÌ‚â‚œâ‚Šâ‚ = autocor(Ïˆ, t)
        ÏÌ‚â‚œâ‚Šâ‚‚ = autocor(Ïˆ, t + 1)
        s = ÏÌ‚â‚œâ‚Šâ‚ + ÏÌ‚â‚œâ‚Šâ‚‚
        s < 0 && break
        Î£Ï += s
        # s < 0 ? break : (Î£Ï += s)
    end
    nÌ‚â‚‘ = n * m / (one(T) + 2Î£Ï)
end

neff(Ï‘::Array{T, 3}) where {T<:Real} = mapslices(neff, Ï‘, dims=(2, 3))

############################################################################################
using Plots
gr(size=(1200,800))
# Example for non-centered parameterization
# Data
y = Float64[28, 8, -3, 7, -1, 1, 18, 12];
Ïƒ = Float64[15, 10, 16, 11, 9, 11, 10, 18];
ÏƒÂ² = abs2.(Ïƒ);

# Integration time and stepsize. These are suggested values -- try some others, keeping ÏµL=1
Ïµ = 0.2
L = 5
N = 1000
N_chains = 8

# Euclidean-Gaussian metric
scale = 15;
Mâ»Â¹ = diagm(ones(10)); Mâ»Â¹[9, 9] = scale^2;
M = diagm(1 ./ diag(Mâ»Â¹));

f_logÏ€ = let y = y, ÏƒÂ² = ÏƒÂ²
    q -> logÏ€q_nc(q, y, ÏƒÂ²)
end;
f_gradlogÏ€ = let y = y, ÏƒÂ² = ÏƒÂ²
    q -> gradlogÏ€_nc(q, y, ÏƒÂ²)
end;

@timev Î¸, E, n, t = hmc(f_logÏ€, f_gradlogÏ€, M, Mâ»Â¹, Ïµ, L, N);

ebfmi(E)

Epq, Eq = microcanonicalenergy(f_logÏ€, Î¸, E);
pâ‚• = histogram([Epq Eq] .- [mean(Epq) mean(Eq)], labels=["E_pq â‰¡ Ï€(E|q)" "E_q â‰¡ Ï€(E)"], alpha=0.5,
               annotations=((0.1, 1.0), text("E-BFMI = $(round(ebfmi(E), digits=3))", 8)),
               xlabel="E - <E>");
savefig(pâ‚•, joinpath(pwd(), "energy.pdf"))

# Multiple chains
@timev chains = [hmc(f_logÏ€, f_gradlogÏ€, M, Mâ»Â¹, Ïµ, L, N) for _ = 1:N_chains];
acprob = getindex.(chains, 3) ./ getindex.(chains, 4)
Î¸ = cat(first.(chains)..., dims=3);
mean(Î¸, dims=(2,3))
E = hcat(getindex.(chains, 2)...);

Ï‘ = originalparam(Î¸[:, (N >> 1 + 1):N, :]);
mean(Ï‘, dims=(2, 3))
# Convergence check
nâ‚‚ = size(Ï‘, 2)
n = nâ‚‚ >> 1
Ï‘â‚‚ = [view(Ï‘, :, 1:n, :);;; view(Ï‘, :, (n + 1):nâ‚‚, :)];
neff(Ï‘â‚‚)
Rhat(Ï‘â‚‚)

ps = map(1:N_chains) do k
    Epq, Eq = microcanonicalenergy(f_logÏ€, Î¸[:, :, k], E[:, k])
    pâ‚• = histogram([Epq Eq] .- [mean(Epq) mean(Eq)], labels=["E_pq â‰¡ Ï€(E|q)" "E_q â‰¡ Ï€(E)"],
                   alpha=0.5,
                   annotations=((0.1, 1.0), text("E-BFMI = $(round(ebfmi(E[:, k]), digits=3))", 8)),
                   xlabel="E - <E>")
    pâ‚•
end;
pâ‚• = plot(ps...);
savefig(pâ‚•, joinpath(pwd(), "energy_$(N_chains)chain.pdf"))

#################### Centered parameterization -- might get stuck due to small Ï„
Ïµ = 0.1
L = 10
N = 1000
N_chains = 8

scale = 15;
M = diagm(fill(1 / scale^2, 10)); M[10, 10] = 1;
Mâ»Â¹ = diagm(1 ./ diag(M));

f_logÏ€_c = let y = y, ÏƒÂ² = ÏƒÂ²
    q -> logÏ€q(q, y, ÏƒÂ²)
end;
f_gradlogÏ€_c = let y = y, ÏƒÂ² = ÏƒÂ²
    q -> gradlogÏ€(q, y, ÏƒÂ²)
end;


# Typically, centered parameterization necessitates a non-small Ï„ starting point, so guarantee it
qáµ¢, páµ¢, táµ¢ = initialize(f_logÏ€_c, f_gradlogÏ€_c, M, Mâ»Â¹, Ïµ, L)
@timev Î¸, E, n, t = hmc(f_logÏ€_c, f_gradlogÏ€_c, qáµ¢, M, Mâ»Â¹, Ïµ, L, N);

ebfmi(E)

Epq, Eq = microcanonicalenergy(f_logÏ€_c, Î¸, E);
pâ‚• = histogram([Epq Eq] .- [mean(Epq) mean(Eq)], labels=["E_pq â‰¡ Ï€(E|q)" "E_q â‰¡ Ï€(E)"], alpha=0.5,
               annotations=((0.1, 1.0), text("E-BFMI = $(round(ebfmi(E), digits=3))", 8)),
               xlabel="E - <E>");
savefig(pâ‚•, joinpath(pwd(), "energy_c.pdf"))

# Multiple chains
@timev chains = [hmc(f_logÏ€_c, f_gradlogÏ€_c,
                     first(initialize(f_logÏ€_c, f_gradlogÏ€_c, M, Mâ»Â¹, Ïµ, L)), # qáµ¢'s
                     M, Mâ»Â¹, Ïµ, L, N) for _ = 1:N_chains];
acprob = getindex.(chains, 3) ./ getindex.(chains, 4)
Î¸ = cat(first.(chains)..., dims=3);
mean(Î¸, dims=(2,3))
E = hcat(getindex.(chains, 2)...);

# Convergence check
neff(Î¸)
Rhat(Î¸)

Ï‘ = Î¸[:, (N >> 1 + 1):N, :];
mean(Ï‘, dims=(2, 3))
# Convergence check
nâ‚‚ = size(Ï‘, 2)
n = nâ‚‚ >> 1
Ï‘â‚‚ = [view(Ï‘, :, 1:n, :);;; view(Ï‘, :, (n + 1):nâ‚‚, :)];
neff(Ï‘â‚‚)
Rhat(Ï‘â‚‚)

ps = map(1:N_chains) do k
    Epq, Eq = microcanonicalenergy(f_logÏ€_c, Î¸[:, :, k], E[:, k])
    pâ‚• = histogram([Epq Eq] .- [mean(Epq) mean(Eq)], labels=["E_pq â‰¡ Ï€(E|q)" "E_q â‰¡ Ï€(E)"],
                   alpha=0.5,
                   annotations=((0.1, 1.0), text("E-BFMI = $(round(ebfmi(E[:, k]), digits=3))", 8)),
                   xlabel="E - <E>")
    pâ‚•
end;
pâ‚• = plot(ps...);
savefig(pâ‚•, joinpath(pwd(), "energy_c_$(N_chains)chain.pdf"))
